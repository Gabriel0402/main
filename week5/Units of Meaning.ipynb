{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5: Units of Meaning\n",
    "\n",
    "The material this week is a bit lighter than usual, and this notebook is accordingly shorter. We'll focus on the segmenter from Peter Norvig's chapter on \"[Natural Language Corpus Data](http://norvig.com/ngrams/ch14.pdf)\", which was covered in the async."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'segment_utils' from 'segment_utils.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "import segment_utils; reload(segment_utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Edit Distance\n",
    "\n",
    "For a good visual demo of edit distance, check out this page:\n",
    "- http://leojiang.com/experiments/levenshtein/ \n",
    "\n",
    "NLTK also includes a basic implementation, which you can access below. The implementation is quite simple: [[source]](http://www.nltk.org/_modules/nltk/metrics/distance.html#edit_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "edit_distance(\"industry\", \"dentistry\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also set it to allow transpositions as an atomic operation, which is handy for spellcheck:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "print edit_distance(\"believe\", \"beleive\")\n",
    "print edit_distance(\"believe\", \"beleive\", transpositions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Segmenter\n",
    "\n",
    "Our text segmenter is based on a language model and dynamic programming. Roughly, we're going to generate a set of candidate segmentations, then pick the most likely one according to our LM.\n",
    "\n",
    "We'll just use a unigram LM for this, \"trained\" on the top 50k words from the Google n-grams corpus. The probabilities are saved to disk, so we need only load them and define a simple function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get unigram probabilities\n",
    "histo = segment_utils.BuildHistogram(\"english_uni_simplified_sorted_top\")\n",
    "\n",
    "# Simple unigram scoring model\n",
    "def Pw(w):\n",
    "  \"\"\"Unigram probabilities with \"stupid\" backoff to character counts.\"\"\"\n",
    "  totals = histo['']\n",
    "  if w in histo:  return math.log(histo[w]) - math.log(totals)\n",
    "  else: return -math.log(totals) - 3*len(w)\n",
    "  \n",
    "def Pwords(words):\n",
    "  return sum(Pw(w) for w in words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that there are $2^{n-1}$ possible segmentations of an $n$-character string. However, we don't need to enumerate all of them. Instead, we'll break the problem into sub-problems:\n",
    "\n",
    "- For each `i = 1, ..., n`, consider the prefix `text[:i+1]`\n",
    "- Recursively compute the best segmentation of the remaining `text[i+1:]`\n",
    "- Return the highest scoring segmentation, according to `Pwords()`\n",
    "\n",
    "We'll use the `@memo` decorator to cache function calls, which lets us avoid expensive re-computation of the same sub-problem. This is equivalent to storing an explicit dynamic programming table, but lets us write the implementation in a recursive style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def splits(text, L=20):\n",
    "  return [(text[:i+1], text[i+1:])\n",
    "          for i in range(min(len(text), L))]\n",
    "\n",
    "@segment_utils.memo\n",
    "def segment(text):\n",
    "  if not text: return []\n",
    "  candidates = ([first]+segment(rem) for first, rem in splits(text))\n",
    "  return max(candidates, key=Pwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['choose', 'spain']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(\"choosespain\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in', 'a', 'hole', 'in', 'the', 'ground', 'there', 'lived', 'a', 'hobbit']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "segment(\"inaholeinthegroundtherelivedahobbit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions\n",
    "\n",
    "**Q:** How many unique calls to `segment(text)` are there? (i.e. what is the size of our cached DP table?)\n",
    "\n",
    "**Q:** What is the total runtime of this algorithm, as implemented above?\n",
    "\n",
    "**Q:** In the above implementation, we treat `Pwords` as a black-box function over a sequence of words. Could we make the segmenter more efficient by exploiting the structure of the language model? What would our optional runtime be in that case?\n",
    "\n",
    "**Bonus Q:** Would your optimization work if we used a bigram language model? Or a trigram model?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
