{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GloVe: Global Vectors for Word Representation (25 points)\n",
        "\n",
        "[GloVe](http://nlp.stanford.edu/projects/glove/) \\[[PDF](http://nlp.stanford.edu/pubs/glove.pdf)\\] is (yet) another way to train word vectors.  Its main advantage relative to Word2Vec is its training speed.\n",
        "\n",
        "## Approach\n",
        "The intuition of the GloVe approach to training word vectors is as follows:\n",
        "\n",
        "1. From the training data, estimate $P(k | word)$.\n",
        "2. Notice that some words, $k$, are far more common than others in the context of $word$.\n",
        "3. In particular, in the table below, notice in the bottom row that $k$'s that are related to ice (vs. steam) result in quite large numbers where as those related to steam (vs. ice) are incredibly low.  Unrelated numbers are about 1.0.\n",
        "\n",
        "<img src=\"glove_table.png\">\n",
        "\n",
        "At a high level then training $F(w_i, w_j, \\tilde{w}_k) = \\frac{P_{ij}}{P_{jk}}$ seems like a useful thing to do.  In this case, F is a (simple) neural network accepting word vectors $w_i$ and $w_j$ for words $i$ and $j$, and a context vector $\\tilde{w}_k$ for word $k$.\n",
        "\n",
        "With some reasonable assumptions about desireable properties of vector embeddings (see Section 3 of the paper), the authors make this more concrete and simplify to a simple objective function based directly on the cooccurrence matrix instead of probabilities:\n",
        "\n",
        "$$J = \\sum\\limits_{i,j}^V f(X_{ij})(w_i^T\\tilde{w}_j + b_i + \\tilde{b}_j - log(X_{ij}))^2$$\n",
        "\n",
        "where $f(.)$ is the weight of the $j$'th word appearing in the $i$th word's context window $X_{ij}$ times.  This weighting function is described in detail immediately before Equation (9) in the paper.\n",
        "\n",
        "Note that $f(0) = 0$ pairs $i,j$ where $X_{ij} = 0$ can be skipped in the sum above.\n",
        "\n",
        "## vs. Word2Vec\n",
        "Similar to Word2Vec, GloVe embeds words in a vector space based on the \"[company it keeps](https://en.wikipedia.org/wiki/John_Rupert_Firth)\" - based on cooccurrance between words in small context windows.  Unlike Word2Vec which repeatedly iterates over the training data one context window at a time, GloVe does a single pass over the training data to collect cooccurrance statistics.  GloVe then trains entirely based on this table of counts.\n",
        "\n",
        "## The Plan\n",
        "In this assignment, you are going to train GloVe models and visualize them.\n",
        "\n",
        "1. Parsing utilities.\n",
        "\n",
        "2. Phrases.  Implement Section 4 (Equation 6) of the [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper.  This allows us to treat \"los angeles\" as a single item in our vocabulary.\n",
        "\n",
        "3. Implement TensorFlow for GloVe & Train embeddings.\n",
        "\n",
        "5. Visualize embeddings.\n",
        "\n",
        "\n",
        "As usual, we begin by importing some useful libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import glove\n",
        "import glove_test\n",
        "import nltk\n",
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import time\n",
        "import unittest\n",
        "import word_stream\n",
        "import word_stream_test\n",
        "import word_utils\n",
        "reload(glove)\n",
        "reload(glove_test)\n",
        "reload(word_stream)\n",
        "reload(word_stream_test)\n",
        "reload(word_utils)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Lower casing sometimes causes more harm than good,\n",
        "# but we do it here anyways in absence of more careful normalization.\n",
        "\n",
        "# The Brown corpus is only 1m tokens.  Test your code with this, then if you want, run with Wikipedia.\n",
        "\n",
        "words = [w.lower() for w in nltk.corpus.brown.words()]\n",
        "\n",
        "# Use first billion bytes of Wikipedia, consisting of 17m tokens.  While this produces better\n",
        "# embeddings, all of the code runs correspondingly longer.  We recommend getting everything to work\n",
        "# with the Brown corpus before trying this.\n",
        "\n",
        "# If you are going to try this using Google Compute Engine, we recommend using the\n",
        "# n1-highcpu-16 (16 vCPUs, 14.4 GB memory) version.\n",
        "\n",
        "# You should not spend time debugging on this instance though or you will find yourself\n",
        "# without GCE credit!\n",
        "\n",
        "# words = open('text8').read().split()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "words[:10], len(words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1.  Parsing Utilities (8 points)\n",
        "\n",
        "In **word_stream.py** (find this in the same directory as this notebook), implement:\n",
        "1.  context_windows\n",
        "2.  cooccurrence_table\n",
        "\n",
        "You may find it helpful to run the follow unit tests to check your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(word_stream)\n",
        "reload(word_stream_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestWordStreams.test_context_windows', word_stream_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(word_stream)\n",
        "reload(word_stream_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestWordStreams.test_cooccurrence_table', word_stream_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.  Phrases (4 points)\n",
        "\n",
        "Implement the function **score_bigram** in **word_stream.py**.\n",
        "Specifically, read the introduction to Section 4 of the [Word2Vec](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf) paper and implement Equation 6."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(word_stream)\n",
        "reload(word_stream_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestWordStreams.test_score_bigram', word_stream_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Use your scoring code to see the best scoring phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "delta = 100  # You can play with this hyperparameter to see how it affects the results.\n",
        "unigrams, bigrams = word_utils.unigram_and_bigram_counts(words)\n",
        "scored_bigrams = sorted(\n",
        "    [(word_stream.score_bigram(bigram, unigrams, bigrams, delta), bigram) for bigram in bigrams],\n",
        "    reverse=True)\n",
        "scored_bigrams[0:500]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This next cell uses the scores computed above and calls grouped_stream which takes a list of words and a set of n-grams and returns the list of words with those n-grams combined into single tokens.\n",
        "\n",
        "e.g. ['the', 'supreme', 'court'] => ['the', 'supreme_court']\n",
        "\n",
        "(You can find more examples in the tests for the function in word_stream_test.py.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# You can leave this cell alone.\n",
        "\n",
        "# If you want to come back afterwards, you can experiment with different phrase_thresholds.\n",
        "# You should set phrase_threshold to a value at which the bigrams in the previous\n",
        "# output start looking less tightly coupled.  grouped_stream below will re-tokenize\n",
        "# a stream of words to consider bigrams scoring above phrase_threshold as a single token.\n",
        "\n",
        "phrase_threshold = 1.0\n",
        "phrases = [bigram for score, bigram in scored_bigrams if score >= phrase_threshold]\n",
        "words = word_utils.grouped_stream(words, phrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Cleanup some variables to recover some memory.\n",
        "del unigrams\n",
        "del bigrams\n",
        "del scored_bigrams\n",
        "del phrases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. TensorFlow for GloVe\n",
        "\n",
        "### Cooccurrence Table\n",
        "In this section, we first build the cooccurrence table with context window of size C."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000  # Amount of vocabulary to keep.  Lower frequency words are mapped to <UNK> (word id: 0).\n",
        "\n",
        "# Map each of the words to a wordid.  Only the most popular VOCAB_SIZE words are kept.\n",
        "vocabulary = word_utils.Vocabulary(words, VOCAB_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Convert the word stream to wordids.\n",
        "# We need to do this because the TensorFlow code you will write in the\n",
        "# next section will use an API that expects indexes into the embedding\n",
        "# matrix, not words.\n",
        "wordids = [vocabulary.to_id(word) for word in words]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Building the cooccurrence table takes a considerable amount of time\n",
        "# with the Wikipedia set.\n",
        "\n",
        "C = 10  # Context window size.\n",
        "ctable = word_stream.cooccurrence_table(wordids, C)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Output top words occurring within the same context.\n",
        "# If everything has worked properly, you should see a considerable number of \"<UNK>\", \"the\", \"of\", and numbers.\n",
        "sorted([(vocabulary.to_word(word), vocabulary.to_word(context_word), count) for word, context_word, count in ctable if count > len(words) / 100], key=lambda x: x[2], reverse=True)[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Shard the table into word lists, context word lists and corresponding counts.\n",
        "# We are going to provide these to TensorFlow as their own entry in the feed_dict,\n",
        "# so we do this separation once, up front.\n",
        "ctable_wids = np.array([word for word, _, _ in ctable])\n",
        "ctable_cwids = np.array([context_word for _, context_word, _ in ctable])\n",
        "ctable_counts = np.array([count for _, _, count in ctable])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Here is what our final data looks like.  It's similar to the table two\n",
        "# cells previous, except instead of words, there are wordids.\n",
        "zip(ctable_wids[:5], ctable_cwids[:5], ctable_counts[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### TensorFlow Graph Setup (12 points)\n",
        "\n",
        "Complete the functions in **glove.py** using the TensorFlow API and then run the corresponding tests in the cells below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(glove)\n",
        "reload(glove_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestGlove.test_embedding_lookup', glove_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(glove)\n",
        "reload(glove_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestGlove.test_example_weight', glove_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reload(glove)\n",
        "reload(glove_test)\n",
        "unittest.TextTestRunner(verbosity=2).run(\n",
        "    unittest.TestLoader().loadTestsFromName(\n",
        "        'TestGlove.test_loss', glove_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Hyperparameters.\n",
        "\n",
        "# You may want to shrink num_examples_to_train to finish debugging\n",
        "# and only run it this long once you are training on Wikipedia.\n",
        "learning_rate = 0.003\n",
        "num_examples_to_train = 3e8\n",
        "batch_size = 100\n",
        "embedding_dim = 300\n",
        "\n",
        "# Construct the training graph.\n",
        "tf.reset_default_graph()\n",
        "\n",
        "wids_ph = tf.placeholder(tf.int32, shape=[None])\n",
        "c_wids_ph = tf.placeholder(tf.int32, shape=[None])\n",
        "counts_ph = tf.placeholder(tf.float32, shape=[None])\n",
        "\n",
        "with tf.variable_scope('word_embeddings'):\n",
        "    word_embeddings, word_bias, word_embed_matrix = (\n",
        "        glove.wordids_to_tensors(wids_ph, embedding_dim, vocabulary.size()))\n",
        "with tf.variable_scope('word_context_embeddings'):\n",
        "    word_context_embeddings, word_context_bias, word_context_embed_matrix = (\n",
        "        glove.wordids_to_tensors(c_wids_ph, embedding_dim, vocabulary.size()))\n",
        "    \n",
        "losses = glove.loss(\n",
        "    word_embeddings, word_bias, word_context_embeddings, word_context_bias,\n",
        "    tf.cast(counts_ph, tf.float32))\n",
        "loss = tf.reduce_mean(losses)\n",
        "\n",
        "# Adam is similar to AdaGrad in that it handles sparse gradients well.\n",
        "# Specifically, you may imagine that some words appear with more context\n",
        "# words than others and with bigger counts.  They therefore are updated\n",
        "# more often and more aggressively (remember the weighting function\n",
        "# you implemented).  Adam backs off updating parameters that it has already\n",
        "# significantly moved around.  (intuitively: the 500th time you backprop\n",
        "# into \"the\", you probably don't have a lot more information to add).\n",
        "#\n",
        "# Here is the original University of Toronto paper detailing the word\n",
        "# done in collaboration with Google DeepMind.\n",
        "# https://arxiv.org/pdf/1412.6980v8.pdf\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "train_op = optimizer.minimize(loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Train the Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Train the embeddings.\n",
        "# Set up the session & initialize variables.\n",
        "sess = tf.Session()\n",
        "sess.run(tf.initialize_all_variables())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Important note:  You do not need to run this cell to completion.\n",
        "# Let it train for 30 minutes or so, then interrupt the kernel and see how good\n",
        "# the nearest-neighbors results look.  Run this cell again to pick up from where\n",
        "# you left off.\n",
        "\n",
        "# An hour on the recommended GCE cloud instance gets reasonably good results.\n",
        "# Two hours cleans up the vectors beautifully.\n",
        "\n",
        "REPORT_LOSS_EVERY = 1000\n",
        "EVAL_BATCH_SIZE = 5000\n",
        "\n",
        "indexes = range(len(ctable_wids))\n",
        "\n",
        "def make_feed_dict(feed_dict_batch_size):\n",
        "    batch_idx = random.sample(indexes, feed_dict_batch_size)\n",
        "    batch_wids = ctable_wids[batch_idx]\n",
        "    batch_cwids = ctable_cwids[batch_idx]\n",
        "    batch_counts = ctable_counts[batch_idx]\n",
        "    return {\n",
        "        wids_ph: batch_wids,\n",
        "        c_wids_ph: batch_cwids,\n",
        "        counts_ph: batch_counts\n",
        "    }\n",
        "\n",
        "num_batches = num_examples_to_train / batch_size + 1\n",
        "\n",
        "print '# training examples:', len(ctable_wids)\n",
        "print '# of epochs:', 1.0 * num_examples_to_train / len(ctable_wids)\n",
        "print '# batches:', num_batches\n",
        "print 'Initial loss:', sess.run(loss, feed_dict=make_feed_dict(EVAL_BATCH_SIZE))\n",
        "\n",
        "current_timer = None\n",
        "for batch in xrange(num_batches):\n",
        "    # Train based on randomly sampled batches of examples.\n",
        "    loss_val, _ = sess.run([loss, train_op], feed_dict=make_feed_dict(batch_size))\n",
        "    \n",
        "    # Do some basic reporting as training progresses.\n",
        "    if batch % REPORT_LOSS_EVERY == 0:\n",
        "        if current_timer:\n",
        "            remaining_reporting_cycles = 1.0 * (num_batches - batch) / REPORT_LOSS_EVERY\n",
        "            cycle_time = time.time() - current_timer\n",
        "            print 'Expected time left:', remaining_reporting_cycles * cycle_time / 60 / 60, 'hours (', cycle_time, 'seconds per', REPORT_LOSS_EVERY, 'batches).'\n",
        "        current_timer = time.time()\n",
        "            \n",
        "        print batch, ':', sess.run(loss, feed_dict=make_feed_dict(EVAL_BATCH_SIZE))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Play! (1 point)\n",
        "\n",
        "Congratulations!  You now have some embeddings.  We only trained a short while over not a lot of text, but these still work reasonably well.\n",
        "\n",
        "If you want more compelling vectors, scroll back up to the top of this notebook and follow the instructions to switch the data source to Wikipedia and execute it again.  Note:  training these vectors for a long time is **not required**, but since you've gotten this far, it takes almost no additional effort to see the result of your hard work below.  The longer you run on the Wikipedia set, the nicer your word vectors will be.  As noted in the previous cell, you can let it run for a while, interrupt the kernel, see how things look, and then run that cell again to have it pick up from where it left off.\n",
        "\n",
        "We have a number of suggestions below to get you started exploring the space.  Feel free to try some of your own."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def find_nn_cos(v, Wv, k=10):\n",
        "    \"\"\"Find nearest neighbors, by cosine distance.\"\"\"\n",
        "    Z = np.linalg.norm(Wv, axis=1) * np.linalg.norm(v)\n",
        "    ds = np.dot(Wv, v.T) / Z\n",
        "    nns = np.argsort(-1*ds)[:k]  # sort descending, take best\n",
        "    return nns, ds[nns]  # word indices, distances\n",
        "\n",
        "def show_nns(v, Wv, vocabulary, k=10):\n",
        "    print \"Nearest neighbors:\"\n",
        "    for i, d in zip(*find_nn_cos(v, Wv, k)):\n",
        "        w = vocabulary.to_word(i)\n",
        "        print \"%.03f : \\\"%s\\\"\" % (d, w)\n",
        "        \n",
        "def word_show_nns(word, Wv, vocabulary, k=10):\n",
        "    show_nns(Wv[vocabulary.to_id(word)], Wv, vocabulary, k)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_embed_matrix_val, word_context_embed_matrix_val = sess.run([word_embed_matrix, word_context_embed_matrix])\n",
        "\n",
        "# As per the paper, we take the average of the word's vector when it's the center word of the window\n",
        "# and the vector when it's found in the context.\n",
        "#\n",
        "# There is some (handwave-y) motivation for why we do this in section 4.2 of GloVe.\n",
        "Wv = word_embed_matrix_val + word_context_embed_matrix_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_show_nns('one', Wv, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_show_nns('king', Wv, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_show_nns('car', Wv, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_show_nns('computer', Wv, vocabulary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "word_show_nns('college', Wv, vocabulary)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [Root]",
      "language": "python",
      "name": "Python [Root]"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
