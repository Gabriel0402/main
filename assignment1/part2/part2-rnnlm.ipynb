{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Assignment 1, Part 2: RNN Language Model\n",
        "**(45 points total)**\n",
        "\n",
        "In this part of the assignment, you'll implement a recurrent neural network language model. This class of models represents the cutting edge in language modeling, and what you implement will be include many of the same features.\n",
        "\n",
        "As a reference, you may want to review the following papers:\n",
        "\n",
        "- [Recurrent neural network based language model](http://www.fit.vutbr.cz/research/groups/speech/publi/2010/mikolov_interspeech2010_IS100722.pdf) (Mikolov, et al. 2010)\n",
        "- [Exploring the Limits of Language Modeling](http://arxiv.org/pdf/1602.02410.pdf) (Jozefowicz, et al. 2016)\n",
        "\n",
        "You'll be writing a fair amount of TensorFlow code, so you may want to review the [Week 1 TensorFlow tutorial](../week1/TensorFlow%20Tutorial.ipynb) or the [Week 4 notebook](../week4/Neural%20Probabilistic%20Language%20Model.ipynb) for review. For documentation on specific functions, consult the [TensorFlow API reference](https://www.tensorflow.org/versions/r0.10/api_docs/python/index.html).\n",
        "\n",
        "Be sure you're using a recent installation of **TensorFlow version 0.9.0 or higher**.\n",
        "\n",
        "#### Note on Indentation\n",
        "\n",
        "This notebook (as well as rnnlm.py) uses 2-space indentation, instead of the 4 spaces that is Python's default. Why? It makes lines shorter, which is handy if you have a lot of nested scopes. Some people will yell at you for doing this, but the instructors don't care either way.\n",
        "\n",
        "You can follow [these instructions](http://jupyter-notebook.readthedocs.io/en/latest/frontend_config.html#example-changing-the-notebook-s-default-indentation) to configure Jupyter to be cool about this. In Chrome, you can open a JavaScript console by going to Menu -> More Tools -> Developer Tools and clicking the \"Console\" tab."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Part 2 Overview\n",
        "\n",
        "- **(a)** RNNLM Inputs and Parameters (written questions)\n",
        "- **(b)** Implementing the RNNLM\n",
        "- **(c)** Training your RNNLM\n",
        "- **(d)** Sampling Sentences\n",
        "- **(e)** Linguistic Properties"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import os, sys, re, json, time, shutil\n",
        "import itertools\n",
        "import collections\n",
        "from IPython.display import display\n",
        "\n",
        "# NLTK for NLP utils and corpora\n",
        "import nltk\n",
        "\n",
        "# NumPy and TensorFlow\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Pandas because pandas are awesome, and for pretty-printing\n",
        "import pandas as pd\n",
        "# Set pandas floating point display\n",
        "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
        "\n",
        "# Helper libraries for this notebook\n",
        "import utils; reload(utils)\n",
        "import vocabulary; reload(vocabulary)\n",
        "import rnnlm; reload(rnnlm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## RNNLM Model\n",
        "\n",
        "![RNNLM](RNNLM.png)\n",
        "\n",
        "Here's the basic spec for our model. We'll use the following notation:\n",
        "\n",
        "- $w^{(i)}$ for the $i^{th}$ word of the sequence (as an integer index)\n",
        "- $x^{(i)}$ for the vector representation of $w^{(i)}$\n",
        "- $h^{(i)}$ for the $i^{th}$ hidden state, with indices as in Section 4.8 of the async.\n",
        "- $o^{(i)}$ for the $i^{th}$ output state, which may or may not be the same as the hidden state\n",
        "\n",
        "Let $ h^{(-1)} = h^{init} $ be an initial state. For an input sequence of $n$ words and $i = 0, ..., n-1$, we have:\n",
        "\n",
        "- **Embedding layer:** $ x^{(i)} = W_{in}[w^{(i)}] $  \n",
        "- **Recurrent layer:** $ (h^{(i)}, o^{(i)}) = \\text{CellFunc}(x^{(i)}, h^{(i-1)}) $\n",
        "- **Output layer:** $ \\hat{P}(w^{(i+1)}) = \\text{softmax}(o^{(i)}W_{out} + b_{out}) $\n",
        " \n",
        "$\\text{CellFunc}$ can be an arbitrary function representing our recurrent cell - it can be a simple RNN cell, or something more complicated like an LSTM.\n",
        "\n",
        "We'll use these as shorthand for important dimensions:\n",
        "- `V` : vocabulary size\n",
        "- `H` : hidden state size = embedding size"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (a) RNNLM Inputs and Parameters (9 points)\n",
        "\n",
        "**(written - no code)** Write your answers in the markdown cell below.  \n",
        "\n",
        "1. Let $\\text{CellFunc}$ be a simple RNN cell (see Section 4.8). Write the functional form in terms of nonlinearities and matrix multiplication. How many parameters (matrix or vector elements) are there for this cell, in terms of `V` and `H`?\n",
        "\n",
        "2. How many parameters are in the embedding layer? In the output layer?\n",
        "\n",
        "3. How many floating point operations are required to compute $\\hat{P}(w^{(i+1)})$ for a given target word $w^{(i+1)}$, assuming $h^{(i-1)}$ is already computed? How about for all target words?\n",
        "\n",
        "4. How does your answer to 3. change if we approximate $\\hat{P}(w^{(i+1)})$ with a sampled softmax with $k$ samples? How about if we use a hierarchical softmax?\n",
        "\n",
        "5. If you have an LSTM with $H = 200$ and use sampled softmax with $k = 100$, what part of the network takes up the most computation time during training?\n",
        "\n",
        "Note: for $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times l}$, computing the matrix product $AB$ takes $O(mnl)$ time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Answers for Part (a)\n",
        "You can use LaTeX to typeset math, e.g. `$ f(x) = x^2 $` will render as $ f(x) = x^2 $.\n",
        "\n",
        "1. *Your text here!*\n",
        "2. *Your text here!*\n",
        "3. *Your text here!*\n",
        "4. *Your text here!*\n",
        "5. *Your text here!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Batching and Truncated BPTT\n",
        "\n",
        "In theory, we model our RNN as operating on a single sequence of arbitrary length. During training, we'd run the RNN over that entire sequence, then backpropagate the errors from all the training labels. \n",
        "\n",
        "In practice, however, it's more common to operate on batches, and to perform *truncated backpropagation* on a fixed-length slice.\n",
        "\n",
        "Batching for an RNN means we'll run several copies of the RNN simultaneously, each with their own hidden state and outputs. Most TensorFlow functions are batch-aware, and expect the batch size as the first dimension.\n",
        "\n",
        "*Truncated backpropagation* means that we'll chop our sequences into blocks of fixed length - say, 20 - and run the RNN for only a fixed number `max_time` steps before we backpropagate errors. We'll still keep the final hidden state so that we can keep computing over the sequence, but we won't backpropagate across this boundary. For example, if our input was the sequence `1 2 3 4 5 6 7 8 9 10 11`, and `max_time=5`, we'd do:\n",
        "\n",
        "- Initialize `h0`\n",
        "- Run on block `[1 2 3 4 5]`\n",
        "- Backprop errors from targets `[2 3 4 5 6]`\n",
        "- Set `h0 = h_final`\n",
        "- Run on block `[6 7 8 9 10]`\n",
        "- Backprop errors from targets `[7 8 9 10 11]`\n",
        "\n",
        "And so on for longer sequences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (b) Implementing the RNNLM (21 points)\n",
        "\n",
        "In order to better manage the model parameters, we'll implement our RNNLM in the `RNNLM` class in `rnnlm.py`. We've given you a skeleton of starter code for this, but the bulk of the implementation is left to you.\n",
        "\n",
        "Particularly, you'll need to implement three functions:\n",
        "- `BuildCoreGraph()` : the main RNN itself\n",
        "- `BuildTrainGraph()` : the training operations, including `train_loss_`, and `train_step_`\n",
        "- `BuildSamplerGraph()` : operations to generate output samples (`pred_samples_`)\n",
        "\n",
        "See `rnnlm.py` for more documentation.\n",
        "\n",
        "### Notes and Tips\n",
        "**`BuildCoreGraph`**  \n",
        "We recommend implementing the `FancyRNNCell` function as a wrapper to construct LSTM cells with (optional) dropout and multi-layer cells.\n",
        "\n",
        "You can experiment with different initializations, but random uniform or Xavier initialization (as in Part 0) should work well.\n",
        "\n",
        "**`BuildTrainGraph`**  \n",
        "You can use the softmax loss from `BuildCoreGraph` here, but we strongly recommend implementing an approximate (e.g. sampled) loss function for `train_loss_`. This will greatly speed up training.\n",
        "\n",
        "For training steps, you can use any optimizer, but we recommend  `tf.train.GradientDescentOptimizer` with gradient clipping (`tf.clip_by_global_norm`) or `tf.train.AdagradOptimizer`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You may find the following API functions useful:\n",
        "- [tf.nn.rnn_cell](https://www.tensorflow.org/versions/r0.9/api_docs/python/rnn_cell.html#neural-network-rnn-cells)\n",
        "- [tf.nn.dynamic_rnn](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#dynamic_rnn)\n",
        "- [tf.nn.sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sparse_softmax_cross_entropy_with_logits)\n",
        "- [tf.nn.sampled_softmax_loss](https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#sampled_softmax_loss)\n",
        "- [tf.multinomial](https://www.tensorflow.org/versions/r0.11/api_docs/python/constant_op.html#multinomial)\n",
        "\n",
        "Additionally, you can expect to make heavy use of [tf.shape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#shape) and [tf.reshape](https://www.tensorflow.org/versions/r0.11/api_docs/python/array_ops.html#reshape). Note especially that you can use `-1` as a dimension in `tf.reshape` to automatically infer the size. For example:\n",
        "```\n",
        "x = tf.zeros([5,10], dtype=tf.float32)\n",
        "x.reshape([-1,])  # shape [50,]\n",
        "x.reshape([1, -1])  # shape [1, 50]\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": false,
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import rnnlm; reload(rnnlm)\n",
        "\n",
        "# Clear old log directory\n",
        "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
        "\n",
        "with tf.Graph().as_default():\n",
        "  tf.set_random_seed(42)\n",
        "\n",
        "  lm = rnnlm.RNNLM(V=10000, H=200, num_layers=2)\n",
        "  lm.BuildCoreGraph()\n",
        "  lm.BuildTrainGraph()\n",
        "  lm.BuildSamplerGraph()\n",
        "\n",
        "  summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
        "                                          tf.get_default_graph())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code above will load your implementation, construct the graph, and write a logdir for TensorBoard. You can bring up TensorBoard with:\n",
        "```\n",
        "tensorboard --logdir tf_summaries --port 6006\n",
        "```\n",
        "As usual, check http://localhost:6006/#graphs to inspect your implementation. Remember, judicious use of `tf.name_scope()` and/or `tf.variable_scope()` will greatly improve the visualization, and make code easier to debug."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (c) Training your RNNLM (5 points)\n",
        "\n",
        "We'll give you data loader functions in **`utils.py`**. They work similarly to the loaders in the Week 4 notebook.\n",
        "\n",
        "Particularly, `utils.batch_generator` will return an iterator that yields minibatches in the correct format. Batches will be of size `[batch_size, max_time]`, and consecutive batches will line up along rows so that the final state of one batch can be used as the initial state for the next.\n",
        "\n",
        "For example, using a toy corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "toy_corpus = \"<s> Mary had a little lamb . <s> The lamb was white as snow . <s>\"\n",
        "toy_corpus = np.array(toy_corpus.split())\n",
        "\n",
        "print \"Input words:\"\n",
        "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
        "for i, (w,y) in enumerate(bi):\n",
        "  utils.pretty_print_matrix(w, cols=[\"w_%d\" % d for d in range(w.shape[1])], dtype=object)\n",
        "\n",
        "print \"Target words:\"\n",
        "bi = utils.batch_generator(toy_corpus, batch_size=2, max_time=4)\n",
        "for i, (w,y) in enumerate(bi):\n",
        "  utils.pretty_print_matrix(y, cols=[\"y_%d\" % d for d in range(w.shape[1])], dtype=object)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Note that the data we feed to our model will be word indices, but the shape will be the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Implement the `run_epoch` function\n",
        "We've given you some starter code for logging progress; fill this in with actual call(s) to `session.run` with the appropriate arguments to run a training step. \n",
        "\n",
        "Be sure to handle the initial state properly at the beginning of an epoch, and remember to carry forward the final state into the next batch.\n",
        "\n",
        "**Note:** we provide a `train=True` flag to enable train mode. If `train=False`, this function can also be used for scoring the dataset - see `score_dataset()` below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def run_epoch(lm, session, batch_iterator, train=False,\n",
        "              verbose=False, tick_s=10, \n",
        "              keep_prob=1.0, learning_rate=0.1):\n",
        "  start_time = time.time()\n",
        "  tick_time = start_time  # for showing status\n",
        "  total_cost = 0.0  # total cost, summed over all words\n",
        "  total_words = 0\n",
        "\n",
        "  if train:\n",
        "    train_op = lm.train_step_\n",
        "    keep_prob = keep_prob\n",
        "    loss = lm.train_loss_\n",
        "  else:\n",
        "    train_op = tf.no_op()\n",
        "    keep_prob = 1.0  # no dropout at test time\n",
        "    loss = lm.loss_  # true loss, if train_loss is an approximation\n",
        "\n",
        "  for i, (w, y) in enumerate(batch_iterator):\n",
        "    cost = 0.0\n",
        "    #### YOUR CODE HERE ####\n",
        "    \n",
        "\n",
        "    \n",
        "    \n",
        "    #### END(YOUR CODE) ####\n",
        "    total_cost += cost\n",
        "    total_words += w.size  # w.size = batch_size * max_time\n",
        "\n",
        "    ##\n",
        "    # Print average loss-so-far for epoch\n",
        "    # If using train_loss_, this may be an underestimate.\n",
        "    if verbose and (time.time() - tick_time >= tick_s):\n",
        "      avg_cost = total_cost / total_words\n",
        "      avg_wps = total_words / (time.time() - start_time)\n",
        "      print \"[batch %d]: seen %d words at %d wps, loss = %.3f\" % (i,\n",
        "          total_words, avg_wps, avg_cost)\n",
        "      tick_time = time.time()  # reset time ticker\n",
        "\n",
        "  return total_cost / total_words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Run Training\n",
        "\n",
        "We'll give you the outline of the training procedure, but you'll need to fill in a call to your `run_epoch` function. \n",
        "\n",
        "At the end of training, we use a `tf.train.Saver` to save a copy of the model to `./tf_saved/rnnlm_trained`. You'll want to load this from disk to work on later parts of the assignment; see **part (d)** for an example of how this is done.\n",
        "\n",
        "#### Tuning Hyperparameters\n",
        "With a sampled softmax loss, the default hyperparameters should train 5 epochs in under 10 minutes on a 16-core Google Cloud Compute instance, and reach a training set perplexity below 200.\n",
        "\n",
        "However, it's possible to do significantly better. Try experimenting with multiple RNN layers (`num_layers` > 1) or a larger hidden state - though you may also need to adjust the learning rate and number of epochs for a larger model.\n",
        "\n",
        "You can also experiment with a larger vocabulary. This will look worse for perplexity, but will be a better model overall as it won't treat so many words as `<unk>`.\n",
        "\n",
        "#### Submitting your model\n",
        "You should submit your trained model along with the assignment. Do:\n",
        "```\n",
        "git add tf_saved/rnnlm_trained tf_saved/rnnlm_trained.meta\n",
        "git commit -m \"Adding trained model.\"\n",
        "```\n",
        "Unless you train a very large model, these files should be < 50 MB and no problem for git to handle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "V = 10000\n",
        "vocab, train_ids, test_ids = utils.load_corpus(\"brown\", split=0.8, V=V, shuffle=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Training parameters\n",
        "max_time = 20\n",
        "batch_size = 50\n",
        "learning_rate = 0.5\n",
        "keep_prob = 1.0\n",
        "num_epochs = 5\n",
        "\n",
        "# Model parameters\n",
        "model_params = dict(V=V, \n",
        "                    H=100, \n",
        "                    num_layers=1)\n",
        "\n",
        "trained_filename = 'tf_saved/rnnlm_trained'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def score_dataset(lm, session, ids, name=\"Data\"):\n",
        "  bi = utils.batch_generator(ids, batch_size=100, max_time=100)\n",
        "  cost = run_epoch(lm, session, bi, \n",
        "                   learning_rate=1.0, keep_prob=1.0, \n",
        "                   train=False, verbose=False, tick_s=3600)\n",
        "  print \"%s: avg. loss: %.03f  (perplexity: %.02f)\" % (name, cost, np.exp(cost))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Will print status every this many seconds\n",
        "print_interval = 5\n",
        "\n",
        "# Clear old log directory\n",
        "shutil.rmtree(\"tf_summaries\", ignore_errors=True)\n",
        "\n",
        "with tf.Graph().as_default(), tf.Session() as session:\n",
        "  # Seed RNG for repeatability\n",
        "  tf.set_random_seed(42)\n",
        "  \n",
        "  with tf.variable_scope(\"model\", reuse=None):\n",
        "    lm = rnnlm.RNNLM(**model_params)\n",
        "    lm.BuildCoreGraph()\n",
        "    lm.BuildTrainGraph()\n",
        "  \n",
        "  session.run(tf.initialize_all_variables())\n",
        "  saver = tf.train.Saver()\n",
        "  \n",
        "  for epoch in xrange(1,num_epochs+1):\n",
        "    t0_epoch = time.time()\n",
        "    bi = utils.batch_generator(train_ids, batch_size, max_time)\n",
        "    print \"[epoch %d] Starting epoch %d\" % (epoch, epoch)\n",
        "    #### YOUR CODE HERE ####\n",
        "    \n",
        "    # Run a training epoch.\n",
        "    \n",
        "    #### END(YOUR CODE) ####\n",
        "    print \"[epoch %d] Completed in %s\" % (epoch, utils.pretty_timedelta(since=t0_epoch))\n",
        "    \n",
        "    ##\n",
        "    # score_dataset will run a forward pass over the entire dataset\n",
        "    # and report perplexity scores. This can be slow (around 1/2 to \n",
        "    # 1/4 as long as a full epoch), so you may want to comment it out\n",
        "    # to speed up training on a slow machine. Be sure to run it at the \n",
        "    # end to evaluate your score.\n",
        "    print (\"[epoch %d]\" % epoch),\n",
        "    score_dataset(lm, session, train_ids, name=\"Train set\")\n",
        "    print (\"[epoch %d]\" % epoch),\n",
        "    score_dataset(lm, session, test_ids, name=\"Test set\")\n",
        "    print \"\"\n",
        "    \n",
        "    # Save a checkpoint\n",
        "    saver.save(session, 'tf_saved/rnnlm', global_step=epoch)\n",
        "    \n",
        "  # Save final model\n",
        "  saver.save(session, trained_filename)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## (d) Sampling Sentences (5 points)\n",
        "\n",
        "If you didn't already in **part (b)**, implement the `BuildSamplerGraph()` method in `rnnlm.py` See the function docstring for more information.\n",
        "\n",
        "#### Implement the `sample_step()` method below (5 points)\n",
        "This should access the Tensors you create in `BuildSamplerGraph()`. Given an input batch and initial states, it should return a vector of shape `[batch_size,]` containing sampled indices for the next word of each batch sequence.\n",
        "\n",
        "Run the method using the provided code to generate 10 sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def sample_step(lm, session, input_w, initial_h):\n",
        "  \"\"\"Run a single RNN step and return sampled predictions.\n",
        "  \n",
        "  Args:\n",
        "    lm : rnnlm.RNNLM\n",
        "    session: tf.Session\n",
        "    input_w : [batch_size] list of indices\n",
        "    initial_h : [batch_size, hidden_dims]\n",
        "  \n",
        "  Returns:\n",
        "    final_h : final hidden state, compatible with initial_h\n",
        "    samples : [batch_size, 1] vector of indices\n",
        "  \"\"\"\n",
        "  #### YOUR CODE HERE ####\n",
        "  # Reshape input to column vector\n",
        "  input_w = np.array(input_w, dtype=np.int32).reshape([-1,1])\n",
        "  \n",
        "  # Run sample ops\n",
        "  \n",
        "  #### END(YOUR CODE) ####\n",
        "  return final_h, samples[:,-1,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Same as above, but as a batch\n",
        "max_steps = 20\n",
        "num_samples = 10\n",
        "random_seed = 42\n",
        "\n",
        "with tf.Graph().as_default(), tf.Session() as session:\n",
        "  # Seed RNG for repeatability\n",
        "  tf.set_random_seed(random_seed)\n",
        "\n",
        "  with tf.variable_scope(\"model\", reuse=None):\n",
        "    lm = rnnlm.RNNLM(**model_params)\n",
        "    lm.BuildCoreGraph()\n",
        "    lm.BuildSamplerGraph()\n",
        "\n",
        "  # Load the trained model\n",
        "  saver = tf.train.Saver()\n",
        "  saver.restore(session, trained_filename)\n",
        "\n",
        "  # Make initial state for a batch with batch_size = num_samples\n",
        "  w = np.repeat([[vocab.START_ID]], num_samples, axis=0)\n",
        "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
        "  # We'll take one step for each sequence on each iteration \n",
        "  for i in xrange(max_steps):\n",
        "    state, y = sample_step(lm, session, w[:,-1:], h)\n",
        "    w = np.hstack((w,y))\n",
        "\n",
        "  # Print generated sentences\n",
        "  for row in w:\n",
        "    for i, word_id in enumerate(row):\n",
        "      print vocab.id_to_word[word_id],\n",
        "        if (i != 0) and (word_id == vocab.START_ID):\n",
        "          break\n",
        "    print \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true
      },
      "source": [
        "## (e) Linguistic Properties (5 points)\n",
        "\n",
        "Now that we've trained our RNNLM, let's test a few properties of the model to see how well it learns linguistic phenomena. We'll do this with a scoring task: given two or more test sentences, our model should score the more plausible (or more correct) sentence with a higher log-probability.\n",
        "\n",
        "We'll define a scoring function to help us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def score_seq(lm, session, seq, vocab):\n",
        "  \"\"\"Score a sequence of words. Returns total log-probability.\"\"\"\n",
        "  padded_ids = vocab.words_to_ids(utils.canonicalize_words([\"<s>\"] + seq, \n",
        "                                                           wordset=vocab.word_to_id))\n",
        "  w = np.reshape(padded_ids[:-1], [1,-1])\n",
        "  y = np.reshape(padded_ids[1:],  [1,-1])\n",
        "  h = session.run(lm.initial_h_, {lm.input_w_: w})\n",
        "  feed_dict = {lm.input_w_:w,\n",
        "               lm.target_y_:y,\n",
        "               lm.initial_h_:h,\n",
        "               lm.dropout_keep_prob_: 1.0}\n",
        "  # Return log(P(seq)) = -1*loss\n",
        "  return -1*session.run(lm.loss_, feed_dict)\n",
        "\n",
        "def load_and_score(inputs, sort=False):\n",
        "  \"\"\"Load the trained model and score the given words.\"\"\"\n",
        "  with tf.Graph().as_default(), tf.Session() as session:  \n",
        "    with tf.variable_scope(\"model\", reuse=None):\n",
        "      lm = rnnlm.RNNLM(**model_params)\n",
        "      lm.BuildCoreGraph()\n",
        "        \n",
        "    # Load the trained model\n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(session, trained_filename)\n",
        "  \n",
        "    if isinstance(inputs[0], str) or isinstance(inputs[0], unicode):\n",
        "      inputs = [inputs]\n",
        "\n",
        "    # Actually run scoring\n",
        "    results = []\n",
        "    for words in inputs:\n",
        "      score = score_seq(lm, session, words, vocab)\n",
        "      results.append((score, words))\n",
        "    \n",
        "    # Sort if requested\n",
        "    if sort: results = sorted(results, reverse=True)\n",
        "    \n",
        "    # Print results\n",
        "    for score, words in results:\n",
        "      print \"\\\"%s\\\" : %.02f\" % (\" \".join(words), score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now we can test as:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sents = [\"once upon a time\",\n",
        "         \"the quick brown fox jumps over the lazy dog\"]\n",
        "load_and_score([s.split() for s in sents])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 1. Number agreement\n",
        "\n",
        "Compare **\"the boy and the girl [are/is]\"**. Which is more plausible according to your model?\n",
        "\n",
        "If your model doesn't order them correctly (*this is OK*), why do you think that might be? (answer in cell below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer to part 1. question(s)\n",
        "\n",
        "*Answer to above question(s).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Type/semantic agreement\n",
        "\n",
        "Compare:\n",
        "- **\"peanuts are my favorite kind of [nut/vegetable]\"**\n",
        "- **\"when I'm hungry I really prefer to [eat/drink]\"**\n",
        "\n",
        "Of each pair, which is more plausible according to your model?\n",
        "\n",
        "How would you expect a 3-gram language model to perform at this example? How about a 5-gram model? (answer in cell below)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "#### YOUR CODE HERE ####\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#### END(YOUR CODE) ####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Answer to part 2. question(s)\n",
        "\n",
        "*Answer to above question(s).*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Adjective ordering (just for fun)\n",
        "\n",
        "Let's repeat the exercise from Week 2:\n",
        "\n",
        "![Adjective Order](adjective_order.jpg)\n",
        "*source: https://twitter.com/MattAndersonBBC/status/772002757222002688?lang=en*\n",
        "\n",
        "We'll consider a toy example (literally), and consider all possible adjective permutations.\n",
        "\n",
        "Note that this is somewhat sensitive to training, and even a good language model might not get it all correct. Why might the NN fail, if the trigram model from week2 was able to solve it?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "prefix = \"I have lots of\".split()\n",
        "noun = \"toys\"\n",
        "adjectives = [\"square\", \"green\", \"plastic\"]\n",
        "inputs = []\n",
        "for adjs in itertools.permutations(adjectives):\n",
        "  words = prefix + list(adjs) + [noun]\n",
        "  inputs.append(words)\n",
        "    \n",
        "load_and_score(inputs, sort=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python [Root]",
      "language": "python",
      "name": "Python [Root]"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
