{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4: Language Modeling II\n",
    "\n",
    "**Note:** We recommend working through the [Week 1 TensorFlow tutorial](../week1/TensorFlow%20Tutorial.ipynb) before starting this notebook.\n",
    "\n",
    "#### Note on training time\n",
    "The NPLM can take a while to train on a slower machine - we clocked it at 10-20 min on a 2-core Cloud Compute instance. \n",
    "\n",
    "If you're using a cloud compute instance, you can add more CPUs without having to re-do setup. With your instance turned off, go to https://console.cloud.google.com/compute/instances, click your instance, and go to \"Edit\". Under machine type, select \"Custom\" and pick 4-8 CPUs and 2 GB of RAM. Make sure you shut down when you're done, and use the Edit menu again to scale back the size to something less expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'vocabulary' from 'vocabulary.pyc'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os, sys, re, json, time\n",
    "import itertools\n",
    "import collections\n",
    "from IPython.display import display\n",
    "\n",
    "# NLTK for NLP utils and corpora\n",
    "import nltk\n",
    "\n",
    "# NumPy and TensorFlow\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Pandas because pandas are awesome, and for pretty-printing\n",
    "import pandas as pd\n",
    "# Set pandas floating point display\n",
    "pd.set_option('float_format', lambda f: \"{0:.04f}\".format(f))\n",
    "\n",
    "# Helper libraries for this notebook\n",
    "import utils\n",
    "reload(utils)\n",
    "import vocabulary\n",
    "reload(vocabulary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this week's notebook, we'll implement the [Neural Probabilistic Language Model (Bengio et al. 2003)](http://machinelearning.wustl.edu/mlpapers/paper_files/BengioDVJ03.pdf). This model is a straightforward extension of n-gram language modeling: it uses a fixed context window, but uses a neural network to predict the next word.\n",
    "\n",
    "Recall that our n-gram mode of order $k+1$ was:\n",
    "$$ P(w_i | w_{i-1}, w_{i-2}, ..., w_0) \\approx P(w_i | w_{i-1}, ..., w_{i-k}) $$\n",
    "Where we estimated the probabilities by smoothed maximum likelihood.\n",
    "\n",
    "For the NPLM, we'll replace that estimate with a neural network predictor that directly learns a mapping from contexts $(w_{i-1}, ..., w_{i-k})$ to a distribution over words $w_i$:\n",
    "\n",
    "$$ P(w_i | w_{i-1}, ..., w_{i-k}) = f(w_i, (w_{i-1}, ..., w_{i-k})) $$\n",
    "\n",
    "Here's what that network will look like:\n",
    "![NPLM architecture](nplm.png)\n",
    "\n",
    "Broadly, there are three parts:\n",
    "1. **Embedding layer**: map words into vector space\n",
    "2. **Hidden layer**: compress and apply nonlinearity\n",
    "3. **Output layer**: predict next word using softmax\n",
    "\n",
    "With modern computers and a couple tricks, we should be able to get a decent model to run in just a few minutes - a far cry from the three weeks it took in 2003!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing our Model\n",
    "\n",
    "To implement the NPLM in TensorFlow, we need to define a Tensor for each model component. As in the tutorial, we'll use variable names that end in an underscore for Tensor objects. We'll also construct the model so it can accept batch inputs, as this will greatly speed up training:\n",
    "\n",
    "Hyperparameters:\n",
    "- `V` : vocabulary size\n",
    "- `M` : embedding size\n",
    "- `N` : context window size\n",
    "- `H` : hidden units\n",
    "\n",
    "Inputs:\n",
    "- `ids_` : (batch_size, N), integer indices for context words\n",
    "- `y_` : (batch_size,), integer indices for target word\n",
    "\n",
    "Model parameters:\n",
    "- `C_` : (V,M), input-side word embeddings\n",
    "- `W1_` : (NxM, H)\n",
    "- `b1_` : (H,)\n",
    "- `W2_` : (H, V)\n",
    "- `W3_` : (NxM, V), matrix for skip-layer connection\n",
    "- `b3_` : (V,)\n",
    "\n",
    "Intermediate states:\n",
    "- `x_` : (batch_size, NxM), concatenated embeddings\n",
    "- `h_` : (batch_size, H), hidden state $= \\tanh(xW_1 + b_1)$\n",
    "- `logit_` : (batch_size, V), $= hW_2 + xW_3 + b_3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "tf.set_random_seed(42)\n",
    "\n",
    "##\n",
    "# Hyperparameters\n",
    "V = 10000\n",
    "M = 30\n",
    "N = 3\n",
    "H = 50\n",
    "\n",
    "# Inputs\n",
    "# Using \"None\" in place of batch size allows \n",
    "# it to be dynamically computed later.\n",
    "with tf.name_scope(\"Inputs\"):\n",
    "    ids_ = tf.placeholder(tf.int32, shape=[None, N], name=\"ids\")\n",
    "    y_ = tf.placeholder(tf.int32, shape=[None], name=\"y\")\n",
    "    \n",
    "with tf.name_scope(\"Embedding_Layer\"):\n",
    "    C_ = tf.Variable(tf.random_uniform([V, M], -1.0, 1.0), name=\"C\")\n",
    "    # embedding_lookup gives shape (batch_size, N, M)\n",
    "    x_ = tf.reshape(tf.nn.embedding_lookup(C_, ids_), \n",
    "                    [-1, N*M], name=\"x\")\n",
    "    \n",
    "with tf.name_scope(\"Hidden_Layer\"):\n",
    "    W1_ = tf.Variable(tf.random_normal([N*M,H]), name=\"W1\")\n",
    "    b1_ = tf.Variable(tf.zeros([H,], dtype=tf.float32), name=\"b1\")\n",
    "    # We could write tf.matmul(x_, W1_) + b1_, \n",
    "    # but tf.add lets us give it a name.\n",
    "    h_ = tf.add(tf.matmul(x_, W1_), b1_, name=\"h\")\n",
    "    \n",
    "with tf.name_scope(\"Output_Layer\"):\n",
    "    W2_ = tf.Variable(tf.random_normal([H,V]), name=\"W2\")\n",
    "    W3_ = tf.Variable(tf.random_normal([N*M,V]), name=\"W3\")\n",
    "    b3_ = tf.Variable(tf.zeros([V,], dtype=tf.float32), name=\"b3\")\n",
    "    # Concat [h x] and [W2 W3]\n",
    "    hx_ = tf.concat(1, [h_, x_], name=\"hx\")\n",
    "    W23_ = tf.concat(0, [W2_, W3_], name=\"W23\")\n",
    "    logits_ = tf.add(tf.matmul(hx_, W23_), b3_, name=\"logits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll add in our usual cross-entropy loss. Recall from async that this is *very* slow for a large vocabulary, and even for a small vocabulary it represents the bulk of the computation time. To speed up training we'll use a sampled softmax loss, as in [Jozefowicz et al. 2016](https://arxiv.org/abs/1602.02410):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Cost_Function\"):\n",
    "    # Sampled softmax loss, for training\n",
    "    per_example_train_loss_ = tf.nn.sampled_softmax_loss(tf.transpose(W23_), b3_, hx_, \n",
    "                                             labels=tf.expand_dims(y_, 1), \n",
    "                                             num_sampled=100, num_classes=V,\n",
    "                                             name=\"per_example_sampled_softmax_loss\")\n",
    "    train_loss_ = tf.reduce_sum(per_example_train_loss_, name=\"sampled_softmax_loss\")\n",
    "    \n",
    "    # Full softmax loss, for scoring\n",
    "    per_example_loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(logits_, y_, name=\"per_example_loss\")\n",
    "    loss_ = tf.reduce_sum(per_example_loss_, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And add training ops. We'll use AdaGrad instead of vanilla SGD, as this tends to converge faster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Training\"):\n",
    "    alpha_ = tf.placeholder(tf.float32, name=\"learning_rate\")\n",
    "    optimizer_ = tf.train.AdagradOptimizer(alpha_)\n",
    "    # train_step_ = optimizer_.minimize(loss_)\n",
    "    train_step_ = optimizer_.minimize(train_loss_)\n",
    "    \n",
    "# Initializer step\n",
    "init_ = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'll add a few ops to do prediction:\n",
    "- `pred_proba_` : (batch_size, V), $ = P(w_i | w_{i-1}, ...)$ for all words $i$\n",
    "- `pred_max` : (batch_size,), id of most likely next word\n",
    "- `pred_random` : (batch_size,), id of a randomly-sampled next word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.name_scope(\"Prediction\"):\n",
    "    pred_proba_ = tf.nn.softmax(logits_, name=\"pred_proba\")\n",
    "    pred_max_ = tf.argmax(logits_, 1, name=\"pred_max\")\n",
    "    pred_random_ = tf.multinomial(logits_, 1, name=\"pred_random\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use TensorBoard to view this graph, even before we run the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary_writer = tf.train.SummaryWriter(\"tf_summaries\", \n",
    "                                        tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a separate terminal, run:\n",
    "```\n",
    "tensorboard --logdir=\"~/w266/week4/tf_summaries\" --port 6006\n",
    "```\n",
    "and go to http://localhost:6006/#graphs\n",
    "\n",
    "It should look something like this:\n",
    "![NPLM graph](nplm-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Corpus\n",
    "\n",
    "As in the original paper, we'll train on the Brown corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.brown\n",
    "\n",
    "token_feed = (utils.canonicalize_word(w) for w in corpus.words())\n",
    "vocab = vocabulary.Vocabulary(token_feed, size=V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 57340 sentences (1.16119e+06 tokens)\n",
      "Sample: \n",
      "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n"
     ]
    }
   ],
   "source": [
    "# Train-test split\n",
    "sentences = list(corpus.sents())\n",
    "print \"Loaded %d sentences (%g tokens)\" % (len(sentences), sum(map(len, sentences)))\n",
    "\n",
    "train_frac = 0.8\n",
    "split_idx = int(train_frac * len(sentences))\n",
    "train_sentences = sentences[:split_idx]\n",
    "dev_sentences = sentences[split_idx:]\n",
    "\n",
    "print \"Sample: \"\n",
    "print \" \".join(train_sentences[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the n-gram models in week 2, we'll canonicalize the words, and truncate our vocabulary to a fixed size. We'll also add a sentence boundary marker `<s>` in between sentences.\n",
    "\n",
    "We'll also need to represent each word as a numerical id. The `Vocabulary` class already makes this mapping for us, so we can just use the utility functions to get our id list.\n",
    "\n",
    "The function below will handle all of this for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample words: \n",
      "<s> <s> <s> the fulton county grand jury said friday an investigation of <unk> recent primary election produced `` no evidence '' that any irregularities took place . <s> the jury further said in <unk> <unk> that the city executive committee , which had over-all charge of the election ,\n",
      "\n",
      "Sample words, as ids: \n",
      "0 0 0 3 5613 655 2288 1640 65 1843 37 2194 6 2 553 1120 1399 1193 16 61 477 17 11 90 9378 217 177 5 0 3 1640 443 65 10 2 2 11 3 242 2056 603 4 39 29 3097 866 6 3 1399 4\n"
     ]
    }
   ],
   "source": [
    "def preprocess_sentences(sentences):\n",
    "    # Add sentence boundaries, canonicalize, and handle unknowns\n",
    "    words = [\"<s>\"]*N + utils.flatten(s + [\"<s>\"] for s in sentences)\n",
    "    words = [utils.canonicalize_word(w, wordset=vocab.word_to_id) \n",
    "             for w in words]\n",
    "    return np.array(vocab.words_to_ids(words))\n",
    "\n",
    "train_ids = preprocess_sentences(train_sentences)\n",
    "dev_ids = preprocess_sentences(dev_sentences)\n",
    "\n",
    "print \"Sample words: \"\n",
    "print \" \".join(vocab.ids_to_words(train_ids[:50]))\n",
    "print \"\"\n",
    "print \"Sample words, as ids: \"\n",
    "print \" \".join(map(str, train_ids[:50]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model is designed to accept batches of data, so we need to do a little re-formatting. We want our input batches to look like the following, where the first $N$ columns are the inputs and the last is the target word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_{i-3}</th>\n",
       "      <th>w_{i-2}</th>\n",
       "      <th>w_{i-1}</th>\n",
       "      <th>target: w_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "      <td>2288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "      <td>2288</td>\n",
       "      <td>1640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w_{i-3}  w_{i-2}  w_{i-1}  target: w_i\n",
       "0        0        3     5613          655\n",
       "1        3     5613      655         2288\n",
       "2     5613      655     2288         1640"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols = [\"w_{i-%d}\" % d for d in range(N,0,-1)] + [\"target: w_i\"]\n",
    "M = np.array([[0,3,5613,655], [3,5613,655,2288], [5613,655,2288,1640]])\n",
    "utils.pretty_print_matrix(M, cols=cols, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll format our entire corpus like this, and then we can just sample blocks from it to get our training minibatches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>w_{i-3}</th>\n",
       "      <th>w_{i-2}</th>\n",
       "      <th>w_{i-1}</th>\n",
       "      <th>target: w_i</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "      <td>2288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5613</td>\n",
       "      <td>655</td>\n",
       "      <td>2288</td>\n",
       "      <td>1640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   w_{i-3}  w_{i-2}  w_{i-1}  target: w_i\n",
       "0        0        0        0            3\n",
       "1        0        0        3         5613\n",
       "2        0        3     5613          655\n",
       "3        3     5613      655         2288\n",
       "4     5613      655     2288         1640"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def build_windows(ids, shuffle=True):\n",
    "    windows = np.zeros((len(ids)-N, N+1), dtype=int)\n",
    "    for i in xrange(N+1):\n",
    "        # First column: first word, etc.\n",
    "        windows[:,i] = ids[i:len(ids)-(N-i)]\n",
    "    if shuffle:\n",
    "        # Shuffle rows\n",
    "        np.random.shuffle(windows)\n",
    "    return windows\n",
    "\n",
    "train_windows = build_windows(train_ids)\n",
    "dev_windows = build_windows(dev_ids)\n",
    "\n",
    "# Check that we got what we want\n",
    "# Just look at the first few IDs for this sample\n",
    "utils.pretty_print_matrix(build_windows(train_ids[:(N+5)], shuffle=False), \n",
    "                          cols=cols, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training time!\n",
    "\n",
    "With our data in array form, we can train our model much like any machine learning model. The code below should look familiar - it's very similar to what we defined for the logistic regression demo. We'll factor out a few operations into helpers, so that the basic structure is clearer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##\n",
    "# Helper functions for training, to reduce boilerplate code\n",
    "\n",
    "def train_batch(session, batch, alpha):\n",
    "    feed_dict = {ids_:batch[:,:-1],\n",
    "                 y_:batch[:,-1],\n",
    "                 alpha_:alpha}\n",
    "    c, _ = session.run([train_loss_, train_step_],\n",
    "                       feed_dict=feed_dict)\n",
    "    return c\n",
    "\n",
    "def score_batch(session, batch):\n",
    "    feed_dict = {ids_:batch[:,:-1],\n",
    "                 y_:batch[:,-1]}\n",
    "    return session.run(loss_, feed_dict=feed_dict)\n",
    "\n",
    "def batch_generator(data, batch_size):\n",
    "    \"\"\"Generate minibatches from data.\"\"\"\n",
    "    for i in xrange(0, len(data), batch_size):\n",
    "        yield data[i:i+batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a single epoch should take around 6-7 minutes on a 2-core Cloud Compute instance, or around 30 seconds on a GTX 980 GPU. You should get good results after just 2-3 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[epoch 1] seen 0 minibatches\n",
      "[epoch 1] seen 1000 minibatches\n",
      "[epoch 1] seen 2000 minibatches\n",
      "[epoch 1] seen 3000 minibatches\n",
      "[epoch 1] seen 4000 minibatches\n",
      "[epoch 1] seen 5000 minibatches\n",
      "[epoch 1] seen 6000 minibatches\n",
      "[epoch 1] seen 7000 minibatches\n",
      "[epoch 1] seen 8000 minibatches\n",
      "[epoch 1] seen 9000 minibatches\n",
      "[epoch 1] seen 10000 minibatches\n",
      "[epoch 1] Completed 10255 minibatches in 0:06:31\n",
      "[epoch 1] Average cost: 6.116\n",
      "\n",
      "[epoch 2] seen 0 minibatches\n",
      "[epoch 2] seen 1000 minibatches\n",
      "[epoch 2] seen 2000 minibatches\n",
      "[epoch 2] seen 3000 minibatches\n",
      "[epoch 2] seen 4000 minibatches\n",
      "[epoch 2] seen 5000 minibatches\n",
      "[epoch 2] seen 6000 minibatches\n",
      "[epoch 2] seen 7000 minibatches\n",
      "[epoch 2] seen 8000 minibatches\n",
      "[epoch 2] seen 9000 minibatches\n",
      "[epoch 2] seen 10000 minibatches\n",
      "[epoch 2] Completed 10255 minibatches in 0:06:35\n",
      "[epoch 2] Average cost: 3.781\n",
      "\n",
      "[epoch 3] seen 0 minibatches\n",
      "[epoch 3] seen 1000 minibatches\n",
      "[epoch 3] seen 2000 minibatches\n",
      "[epoch 3] seen 3000 minibatches\n",
      "[epoch 3] seen 4000 minibatches\n",
      "[epoch 3] seen 5000 minibatches\n",
      "[epoch 3] seen 6000 minibatches\n",
      "[epoch 3] seen 7000 minibatches\n",
      "[epoch 3] seen 8000 minibatches\n",
      "[epoch 3] seen 9000 minibatches\n",
      "[epoch 3] seen 10000 minibatches\n",
      "[epoch 3] Completed 10255 minibatches in 0:06:21\n",
      "[epoch 3] Average cost: 3.493\n"
     ]
    }
   ],
   "source": [
    "# One epoch = one pass through the training data\n",
    "num_epochs = 3\n",
    "batch_size = 100\n",
    "alpha = 0.1  # learning rate\n",
    "print_every = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "session = tf.Session()\n",
    "session.run(init_)\n",
    "\n",
    "t0 = time.time()\n",
    "for epoch in xrange(1,num_epochs+1):\n",
    "    t0_epoch = time.time()\n",
    "    epoch_cost = 0.0\n",
    "    print \"\"\n",
    "    for i, batch in enumerate(batch_generator(train_windows, batch_size)):\n",
    "        if (i % print_every == 0):\n",
    "            print \"[epoch %d] seen %d minibatches\" % (epoch, i)\n",
    "        \n",
    "        epoch_cost += train_batch(session, batch, alpha)\n",
    "\n",
    "    avg_cost = epoch_cost / len(train_windows)\n",
    "    print \"[epoch %d] Completed %d minibatches in %s\" % (epoch, i, utils.pretty_timedelta(since=t0_epoch))\n",
    "    print \"[epoch %d] Average cost: %.03f\" % (epoch, avg_cost,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring\n",
    "\n",
    "We'll score our model the same as the n-gram model, by computing perplexity over the dev set. Recall that perplexity is just the exponentiated average cross-entropy loss:\n",
    "\n",
    "$$ \\text{Perplexity} = \\left( \\prod_i \\frac{1}{Q(x_i)} \\right)^{1/N} = \\left( \\prod_i 2^{- \\log_2 Q(x_i)} \\right)^{1/N} = 2^{\\left(\\frac{1}{N} \\sum_i -\\log_2 Q(x_i)\\right)} = 2^{\\tilde{CE}(P,Q)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def score_dataset(data):\n",
    "    total_cost = 0.0\n",
    "    for batch in batch_generator(data, 1000):\n",
    "        total_cost += score_batch(session, batch)\n",
    "\n",
    "    avg_cost = total_cost / len(data)\n",
    "    return avg_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set perplexity: 38.731\n",
      "Dev set perplexity: 42.792\n"
     ]
    }
   ],
   "source": [
    "print \"Train set perplexity: %.03f\" % 2**score_dataset(train_windows)\n",
    "print \"Dev set perplexity: %.03f\" % 2**score_dataset(dev_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty good! Note that these numbers aren't directly comparable to the literature, since we made the task easier by lowercasing everything, canonicalizing digits, and treating a fairly large number of words as an `<unk>` token.\n",
    "\n",
    "We can remove some of this handicap by looking at our perplexity on non-`<unk>` target words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filtered dev set perplexity: 48.361\n"
     ]
    }
   ],
   "source": [
    "filtered_dev_windows = dev_windows[dev_windows[:,-1] != vocab.UNK_ID]\n",
    "print \"Filtered dev set perplexity: %.03f\" % 2**score_dataset(filtered_dev_windows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling\n",
    "\n",
    "We can sample sentences from the model much as we did with n-gram models. We'll use the `pred_random_` op that we defined before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> <s> <s> both success of the old material for a good was exactly after the <unk> <unk> undertaken to sympathies . <s>\n",
      "[23 tokens; log P(seq): 94.61, per-token: 4.51]\n",
      "\n",
      "<s> <s> <s> the world who therefore . <s>\n",
      "[9 tokens; log P(seq): 23.40, per-token: 3.34]\n",
      "\n",
      "<s> <s> <s> the additional plants , of an hour '' , relations , <unk> annual <unk> . <s>\n",
      "[19 tokens; log P(seq): 69.72, per-token: 4.10]\n",
      "\n",
      "<s> <s> <s> ranks , waving slightly kate put it to freedom , we forms , roll his enjoyment . <s>\n",
      "[21 tokens; log P(seq): 111.63, per-token: 5.88]\n",
      "\n",
      "<s> <s> <s> took short , from <unk> , eileen , for chickens , mr. convincing sleeves . <s>\n",
      "[19 tokens; log P(seq): 76.43, per-token: 4.50]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def predict_next(session, seq):\n",
    "    feed_dict={ids_:np.array([seq[-N:]])}\n",
    "    next_id = session.run(pred_random_, feed_dict=feed_dict)\n",
    "    return next_id[0][0]\n",
    "\n",
    "def score_seq(session, seq):\n",
    "    # Some gymnastics to generate windows for scoring\n",
    "    windows = [seq[i:i+N+1] for i in range(len(seq)-(N+1))]\n",
    "    return score_batch(session, np.array(windows))\n",
    "\n",
    "max_length = 30\n",
    "num_sentences = 5\n",
    "\n",
    "for _ in range(num_sentences):\n",
    "    seq = [vocab.word_to_id[\"<s>\"]]*N  # init N+1-gram model\n",
    "    for i in range(max_length):\n",
    "        seq.append(predict_next(session, seq))\n",
    "        if seq[-1] == vocab.word_to_id[\"<s>\"]: break\n",
    "    print \" \".join(vocab.ids_to_words(seq))\n",
    "    score = score_seq(session, seq)\n",
    "    print \"[%d tokens; log P(seq): %.02f, per-token: %.02f]\" % (len(seq), score, \n",
    "                                                                score/(len(seq)-2))\n",
    "    print \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
