{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity and Information Theory\n",
    "Ian Tenney, September 8, 2016\n",
    "\n",
    "Consider our language modeling task, where the goal is to predict the next word in a sentence:\n",
    "```\n",
    "I have a pet ____\n",
    "```\n",
    "We want to model, for all possible words $w$:\n",
    "$$ P(w_i = w | w_{i-1}, w_{i-2}, ..., w_0) $$\n",
    "\n",
    "Of course, we can't predict with certainty what word should go here, and there's plenty of valid options. If we knew the true distribution, we could precisely say what the probabilities should be - but we don't, so we'll have to estimate it from data.\n",
    "\n",
    "Suppose our corpus is ten sentences:\n",
    "\n",
    "```\n",
    "I have a pet dog\n",
    "I have a pet dog\n",
    "I have a pet dog\n",
    "I have a pet dog\n",
    "I have a pet cat\n",
    "I have a pet cat\n",
    "I have a pet cat\n",
    "I have a pet cat\n",
    "I have a pet gecko\n",
    "I have a pet rock\n",
    "```\n",
    "\n",
    "We can get our maximum likelihood estimate from counting words. We'll use $Q$ here for our model distributions; you'll see why soon:\n",
    "$$ Q(\\text{dog}\\ |\\ \\text{I have a pet}) = 4/10 = 0.4$$\n",
    "$$ Q(\\text{cat}\\ |\\ \\text{I have a pet}) = 4/10 = 0.4$$\n",
    "$$ Q(\\text{gecko}\\ |\\ \\text{I have a pet}) = 1/10 = 0.1$$\n",
    "$$ Q(\\text{rock}\\ |\\ \\text{I have a pet}) = 1/10 = 0.1$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q(dog)=0.40  Q(cat)=0.40  Q(gecko)=0.10  Q(rock)=0.10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "words = ['dog', 'cat', 'gecko', 'rock']\n",
    "c = np.array([4,4,1,1])\n",
    "q = c / np.sum(c, dtype=float)\n",
    "print \"  \".join(\"Q(%s)=%.02f\" % wp for wp in zip(words, q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy\n",
    "\n",
    "Entropy is a measure of information. When we're dealing with random variables, entropy tells us how much information is \"contained\" in an instance of that variable.\n",
    "\n",
    "For example, for a variable that's always equal to 1, it doesn't take any information at all to specify the value. So, entropy is zero bits.\n",
    "\n",
    "For a variable that's equal to 0 or 1 with equal probability, we need one bit. For one that can be in `{0,1,2,3}` with equal probability, we need two bits to specify the value: `00`, `01`, `10`, or `11`.\n",
    "\n",
    "What if we get a skewed distribution? It turns out we can compress things a bit. Let's try a [Huffman code](https://en.wikipedia.org/wiki/Huffman_coding): we'll assign shorter codes to the more common entries, and longer ones to things we see less often:  \n",
    "`\"dog\": 0`  \n",
    "`\"cat\": 10`  \n",
    "`\"gecko\": 110`  \n",
    "`\"rock\": 111`  \n",
    "Now on average, we need:  \n",
    "\n",
    "$$ E[\\text{bits}] = Q(\\text{dog}) \\cdot 1 + Q(\\text{cat}) \\cdot 2 + Q(\\text{gecko}) \\cdot 3 + Q(\\text{rock}) \\cdot 3 = 1.8\\ \\text{bits} $$\n",
    "\n",
    "So we didn't need two full bits after all! Because some elements are more frequent, there's less information we need to store overall.\n",
    "\n",
    "We can formalize this with the notion of **Entropy**. According to the [Shannon source coding theorem](https://en.wikipedia.org/wiki/Shannon%27s_source_coding_theorem), the optimal number of bits for each symbol is $-\\log_2(q(x))$. Suppose we had that. Then the expected number of bits would be:\n",
    "\n",
    "$$ E[\\text{bits}] = H(Q) = -\\sum_x Q(x) \\log_2 Q(x) $$\n",
    "\n",
    "where for our example $x \\in \\{\\text{dog, cat, gecko, rock}\\}$. Let's calculate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.7219280948873623"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use q for this, as it's a predicted distribution\n",
    "sum(-q * np.log2(q))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our Huffman code gets close to optimal. In this case, it's just limited by discretization - we have to use a whole number of bits, so we might need to round up from the optimum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Entropy and KL Divergence\n",
    "\n",
    "Suppose we have our language model, trained to predict the above distribution. Of course, the corpus was just a sample from some underlying true distribution. Suppose we know that of all pet owners, 40% have dogs, 50% have cats, and only 5% each have geckos or pet rocks.\n",
    "\n",
    "Now we can ask, how well are we approximating our true distribution with our model? More precisely: suppose we come up with a code optimized for our model distribution $Q$, like the one above. How many extra bits will we need if we use this code on the true distribution $P$?\n",
    "\n",
    "Using our *optimal* code from $Q$ on samples from $P$, we get the **cross-entropy** between $P$ and $Q$:\n",
    "\n",
    "$$ CE(P,Q) = -\\sum_x P(x) \\log_2 Q(x) $$\n",
    "\n",
    "The number of bits we would need with an optimal code from $P$, on $P$, would be the **entropy** of P:\n",
    "\n",
    "$$ H(P) = -\\sum_x P(x) \\log_2 P(x) $$\n",
    "\n",
    "Now the difference, how much \"extra\" information we would need - or equivalently, the information lost when we approximate $P$ by $Q$, is the **Kullback-Liebler (KL) divergence:**\n",
    "\n",
    "$$ D_{KL}(P||Q) = CE(P,Q) - H(P) = -\\sum_x P(x) \\log_2 Q(x) + \\sum_x P(x) \\log_2 P(x) $$\n",
    "$$ D_{KL}(P||Q) = \\sum_x P(x) \\log_2 \\frac{P(x)}{Q(x)} $$\n",
    "\n",
    "Note that KL divergence is not symmetric; it assumes one \"true\" distribution ($P$) and an approximation to it ($Q$).\n",
    "\n",
    "Also observe that the difference between the cross-entropy and the KL divergence only depends on $P$. So if $P$ is a fixed \"true\" distribution that we want to approximate, then it's equivalent to optimize either the KL divergence or the cross-entropy. The latter has a simpler form, so that's what we use in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   CE(P,Q) = 1.52193\n",
      "      H(P) = 1.46096\n",
      "D_KL(P||Q) = 0.06096\n"
     ]
    }
   ],
   "source": [
    "p = np.array([0.4, 0.5, 0.05, 0.05])\n",
    "ce_pq = sum(-p * np.log2(q))\n",
    "h_p = sum(-p * np.log2(p))\n",
    "dkl_pq = ce_pq - h_p\n",
    "print \"   CE(P,Q) = %.05f\" % ce_pq\n",
    "print \"      H(P) = %.05f\" % h_p\n",
    "print \"D_KL(P||Q) = %.05f\" % dkl_pq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The KL divergence isn't very high in this toy example, because our sample wasn't that far off from the true distribution. More interestingly though, note that the cross-entropy loss is still fairly high. What if we had a perfect model, and set $Q = P$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CE(P,P) = 1.46096\n",
      "D_KL(P||P) = 0.00000\n"
     ]
    }
   ],
   "source": [
    "ce_pp = sum(-p * np.log2(p))\n",
    "print \"CE(P,P) = %.05f\" % ce_pp\n",
    "print \"D_KL(P||P) = %.05f\" % (ce_pp - h_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KL divergence is 0, but we still get a loss of 1.46 - not great! But we can't possibly do better than the true distribution; there's just 1.46 bits of inherent uncertainty in the next word here. This is exactly the same situation you might have seen with noisy training labels, where we can't do better than the [Bayes error rate](https://en.wikipedia.org/wiki/Bayes_error_rate)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perplexity\n",
    "\n",
    "For machine learning, it's not always useful to think about \"encoding\". Instead, we're just interested in how well our predictions match up with *data*. We don't get to know the true value of $P(x)$ - instead, we can evaluate on a sample from it. \n",
    "\n",
    "Let $x_i : i = 1,...,N$ be our samples. We can approximate the cross-entropy as an expectation over the true distribution:\n",
    "\n",
    "$$ CE(P,Q) = -\\sum_x P(x) \\log_2 Q(x) = E_{P}\\left[ -\\log_2 Q(x) \\right] \\approx \\frac{1}{N} \\sum_i -\\log_2 Q(x_i) $$\n",
    "\n",
    "This is our usual machine learning objective:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated CE(P,Q) = 1.92193\n"
     ]
    }
   ],
   "source": [
    "test_set = [\n",
    "\"I have a pet dog\",\n",
    "\"I have a pet cat\",\n",
    "\"I have a pet dog\",\n",
    "\"I have a pet dog\",\n",
    "\"I have a pet cat\",\n",
    "\"I have a pet cat\",\n",
    "\"I have a pet gecko\",\n",
    "\"I have a pet cat\",\n",
    "\"I have a pet gecko\",\n",
    "\"I have a pet rock\",\n",
    "]\n",
    "\n",
    "word_to_q = dict(zip(words, q))\n",
    "q_xi = np.array([word_to_q[s.split()[-1]] for s in test_set])\n",
    "est_ce = np.mean(-np.log2(q_xi))\n",
    "print \"Estimated CE(P,Q) = %.05f\" % est_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For language modeling purposes, we tend to prefer an equivalent, but easier to interpret measure: **perplexity**. For the cross-entropy loss, we took the *arithemetic mean* of the *negative log probabilities*. We can instead think of the *inverse probabilities*, and take the *geometric* mean:\n",
    "\n",
    "$$ \\text{Perplexity} = \\left( \\prod_i \\frac{1}{Q(x_i)} \\right)^{1/N} $$\n",
    "\n",
    "You can think of this as averaging how confused the model is, over all the words (hence the name, \"perplexity\"). The more probability the model puts on the right word, the lower the perplexity - down to, at best, 1. As a mental model, you can think of this as how many words the model thinks are plausible at each point: suppose the model suggests $k$ words - including the correct one - and assigns each $Q(x) = 1/k$. Then the perplexity would be $1/Q(x) = k$.\n",
    "\n",
    "Just as cross-entropy won't go below $H(P)$, perplexity won't ever go below a certain point - even the best model should be able to predict several plausible candidates.\n",
    "\n",
    "In fact, perplexity is just the exponentiated cross-entropy loss:\n",
    "\n",
    "$$ \\text{Perplexity} = \\left( \\prod_i \\frac{1}{Q(x_i)} \\right)^{1/N} = \\left( \\prod_i 2^{- \\log_2 Q(x_i)} \\right)^{1/N} = 2^{\\left(\\frac{1}{N} \\sum_i -\\log_2 Q(x_i)\\right)} \\approx 2^{CE(P,Q)}$$\n",
    "\n",
    "If we were to multiply all the probabilities together directly, we'd get a numerical underflow, so it's much more common to compute perplexity in log-space and take the exponent at the end:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 3.79\n"
     ]
    }
   ],
   "source": [
    "est_ce = np.mean(-np.log2(q_xi))\n",
    "print \"Perplexity: %.02f\" % 2**est_ce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One last note: Conditional probabilities\n",
    "\n",
    "Just like when we average our loss over training examples with different features, we average perplexity over predicted words with different contexts. Just replace all samples $x_i$ with (sample, context) pairs $x_i, c_i$, where $c_i = (w_{i-1}, w_{i-2}, ...)$, and replace $P(x_i)$ and $Q(x_i)$ with the conditional probabilities $P(x_i | c_i)$ and $Q(x_i | c_i)$, and all the above results still hold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
