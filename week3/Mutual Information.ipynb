{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Theory: Mutual Information\n",
    "Ian Tenney, September 10, 2016\n",
    "\n",
    "We introduced this concept briefly in section last week, but it's worth expanding upon a bit here.\n",
    "\n",
    "**Mutual information** is a general way of measuring the relationship between two random variables. More precisely, it tells us how much *information* (in bits) each variable tells us about the other. In this way it's similar to the idea of correlation, but it's not limited to real-valued variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider a simple corpus, with names for the pets. We'll compute a co-occurrence matrix between pets (rows) and names (cols):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chloe</th>\n",
       "      <th>Ozzie</th>\n",
       "      <th>Jinx</th>\n",
       "      <th>Fritz</th>\n",
       "      <th>Remy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gecko</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Chloe  Ozzie  Jinx  Fritz  Remy\n",
       "dog        1      1     0      0     0\n",
       "cat        1      0     1      1     0\n",
       "gecko      0      0     0      0     1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "corpus = [\n",
    "\"I have a pet dog named Chloe\",\n",
    "\"I have a pet dog named Ozzie\",\n",
    "\"I have a pet cat named Jinx\",\n",
    "\"I have a pet cat named Fritz\",\n",
    "\"I have a pet cat named Chloe\",\n",
    "\"I have a pet gecko named Remy\",\n",
    "]\n",
    "\n",
    "pets = [\"dog\", \"cat\", \"gecko\"]\n",
    "pet_to_row = {w:i for i,w in enumerate(pets)}\n",
    "names = [\"Chloe\", \"Ozzie\", \"Jinx\", \"Fritz\", \"Remy\"]\n",
    "name_to_col = {w:i for i,w in enumerate(names)}\n",
    "\n",
    "Cxy = np.zeros((3,5))\n",
    "pairs = [(s.split()[-3], s.split()[-1]) for s in corpus]\n",
    "for pet, name in pairs:\n",
    "    i, j = pet_to_row[pet], name_to_col[name]\n",
    "    Cxy[i,j] += 1\n",
    "\n",
    "# Pretty-print function\n",
    "def pretty_print_matrix(M, rows=pets, cols=names, dtype=float):\n",
    "    display(pd.DataFrame(M, index=rows, columns=cols, dtype=dtype))\n",
    "    \n",
    "# Pretty-print with headers\n",
    "pretty_print_matrix(Cxy, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to know: how much does one word (e.g. the pet type) tell us about another (e.g. the pet's name)?\n",
    "\n",
    "Let's look at a single pair of words. Suppose we know the pet is a `dog`. How much more likely is it to be named `Ozzie`? Let's measure the ratio of probabilities:\n",
    "\n",
    "$$ \\frac{P(\\text{\"Ozzie\"}\\ |\\ \\text{\"dog\"})}{P(\\text{\"Ozzie\"})} = \\frac{1/2}{1/6} = 3$$\n",
    "\n",
    "As usual, we'll take the log to get units of information:\n",
    "\n",
    "$$ \\text{PMI}(\\text{\"Ozzie\"},\\text{\"dog\"}) = \\log_2 \\frac{P(\\text{\"Ozzie\"}\\ |\\ \\text{\"dog\"})}{P(\\text{\"Ozzie\"})} = \\log_2 (3) $$\n",
    "\n",
    "This quantity is known as **pointwise mutual information** (PMI). In general form:  \n",
    "\n",
    "$$ \\text{PMI}(x,y) = \\log_2 \\frac{P(x | y)}{P(x)} = \\log_2 \\frac{P(x | y)P(y)}{P(x)P(y)} = \\log_2 \\frac{P(x,y)}{P(x)P(y)}  $$  \n",
    "The value of PMI is the same whichever side we condition on - unlike cross-entropy or KL divergence, this is symmetric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chloe</th>\n",
       "      <th>Ozzie</th>\n",
       "      <th>Jinx</th>\n",
       "      <th>Fritz</th>\n",
       "      <th>Remy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0.584963</td>\n",
       "      <td>1.584963</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>-inf</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-inf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gecko</th>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>-inf</td>\n",
       "      <td>2.584963</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Chloe     Ozzie      Jinx     Fritz      Remy\n",
       "dog    0.584963  1.584963      -inf      -inf      -inf\n",
       "cat    0.000000      -inf  1.000000  1.000000      -inf\n",
       "gecko      -inf      -inf      -inf      -inf  2.584963"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Pxy = Cxy / np.sum(Cxy)\n",
    "Px = Pxy.sum(axis=1)  # sum each row\n",
    "Py = Pxy.sum(axis=0)  # sum each column\n",
    "\n",
    "# Pointwise mutual information\n",
    "# Note: np.outer(Px,Py)[i,j] = Px[i] * Py[j]\n",
    "PMI_xy = np.log2(Pxy / np.outer(Px, Py))\n",
    "pretty_print_matrix(PMI_xy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mutual Information\n",
    "\n",
    "The mutual information (MI) is just the expectation of PMI, over all possible pairs $(x,y)$:\n",
    "\n",
    "$$ I(X,Y) = E_{x,y}\\left[\\text{PMI}(x,y)\\right] = \\sum_{x,y} P(x,y) \\log_2 \\frac{P(x,y)}{P(x)P(y)}$$\n",
    "\n",
    "Let's compute it over our corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12581458369\n"
     ]
    }
   ],
   "source": [
    "# The -inf values should be canceled by Pxy = 0\n",
    "# Need np.nansum to ignore nan = (-inf * 0), since these values should really be zero.\n",
    "I_xy = np.nansum(Pxy * PMI_xy)\n",
    "print I_xy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we interpret this? It's expressed in bits, so we can say that on average, there is 1.12 bits of information in the correlation between pet and name.\n",
    "\n",
    "More formally, we can expand the sum and write MI in terms of entropy:\n",
    "\n",
    "$$ I(X,Y) = \\sum_{x,y} P(x,y) \\log \\frac{P(x,y)}{P(x)P(y)} $$\n",
    "$$ = \\sum_{x,y} P(x,y) \\log_2 P(x,y) - \\sum_{x,y} P(x,y) \\log_2 P(x) - \\sum_{x,y} P(x,y) \\log_2 P(y) $$\n",
    "$$ = \\sum_{x,y} P(x,y) \\log_2 P(x,y) - \\sum_{x} P(x) \\log_2 P(x) - \\sum_{y} P(y) \\log_2 P(y) $$\n",
    "$$ = - H(X,Y) + H(X) + H(Y) $$\n",
    "\n",
    "Recall that $H(X)$ is the information (entropy) of $X$: so the mutual information is the difference between how much information we would need to specify $X$ and $Y$ separately, we need to specify them jointly as pairs $(X,Y)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H(X) = 1.4591\n",
      "H(Y) = 2.2516\n",
      "H(X,Y) = 2.5850\n",
      "I(X,Y) = 1.1258\n"
     ]
    }
   ],
   "source": [
    "# Use np.nansum again to ignore nan = (-inf * 0)\n",
    "Hx = np.nansum(-Px * np.log2(Px))\n",
    "Hy = np.nansum(-Py * np.log2(Py))\n",
    "Hxy = np.nansum(-Pxy * np.log2(Pxy))\n",
    "print \"H(X) = %.04f\" % Hx\n",
    "print \"H(Y) = %.04f\" % Hy\n",
    "print \"H(X,Y) = %.04f\" % Hxy\n",
    "print \"I(X,Y) = %.04f\" % (Hx + Hy - Hxy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
