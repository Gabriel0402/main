{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6: Information Retrieval\n",
    "\n",
    "For this week's notebook, we'll implement a (very) simple search engine with ranked retrieval. Our scoring model will be based on TF-IDF:\n",
    "\n",
    "$$ \\text{TF-IDF}_{t,d} = \\left(1 + \\log_{10}(\\text{TF}_{t,d})\\right) \\cdot \\log_{10}\\left( \\frac{N}{\\text{DF}_{t}}\\right) $$\n",
    "\n",
    "Where for each document, we'll compute the summed TF-IDF scores for each term in the query:\n",
    "\n",
    "$$ \\text{score}(q,d) = \\sum_{t \\in q} \\text{TF-IDF}_{t,d} $$\n",
    "\n",
    "For a small corpus, it's relatively trivial to implement this in-memory. To better reflect the real-world use, where many machines are needed to construct and serve the index, we'll build our model using a MapReduce paradigm, in three stages:\n",
    "\n",
    "1. **Map stage**: extract `(word, doc_id)` pairs\n",
    "2. **Reduce stage 1**: sum identical tuples to get `(word, doc_id, `$\\text{TF}_{t,d}$`)`\n",
    "3. **Reduce stage 2**: build word index of sparse TF-IDF vectors\n",
    "\n",
    "We'll just use Python functions for the basic `map`, `groupby`, and `reduce` primitives, but the same control flow can be adapted to run as a large-scale distributed system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys, os, time, re\n",
    "import collections, itertools\n",
    "\n",
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "import utils; reload(utils)\n",
    "\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our corpus, we'll use the [Reuters corpus](http://www.nltk.org/book/ch02.html#reuters-corpus) included with NLTK. This corpus consists of about 12,000 short news articles from Reuters in 1987.\n",
    "\n",
    "The `paras()` function will return individual articles which we can index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corpus = nltk.corpus.reuters\n",
    "documents = corpus.paras()  # lazy iterator! streams from disk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Stage\n",
    "\n",
    "In this stage, we'll make a single pass through the documents. We'll build a document index `doc_id -> text`, and emit tuples of `(word, doc_id)` that we'll use to build the term index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Map stage completed in 0:00:06\n",
      "Emitted 1720917 words from 11887 documents\n"
     ]
    }
   ],
   "source": [
    "doc_index = {}\n",
    "map_output = []\n",
    "\n",
    "word_count = 0\n",
    "doc_count = 0\n",
    "t0 = time.time()\n",
    "for doc_id, document in enumerate(documents):\n",
    "  for word in utils.flatten(document):\n",
    "    map_output.append((word.lower(), doc_id))\n",
    "    word_count += 1\n",
    "  # Store document text\n",
    "  doc_index[doc_id] = utils.flatten(document)\n",
    "  doc_count += 1\n",
    "\n",
    "print \"Map stage completed in %s\" % utils.pretty_timedelta(since=t0)\n",
    "print \"Emitted %d words from %d documents\" % (word_count, doc_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'asian', 0),\n",
       " (u'exporters', 0),\n",
       " (u'fear', 0),\n",
       " (u'damage', 0),\n",
       " (u'from', 0),\n",
       " (u'u', 0),\n",
       " (u'.', 0),\n",
       " (u's', 0),\n",
       " (u'.-', 0),\n",
       " (u'japan', 0)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Stage 1\n",
    "\n",
    "In the first reduce stage, we'll simply de-dupe to convert our emitted tuples into term counts. So:\n",
    "\n",
    "`(\"foo\", 42)`  \n",
    "`(\"foo\", 42)`  \n",
    "`(\"bar\", 7)`\n",
    "\n",
    "will reduce to:\n",
    "\n",
    "`(\"foo\", 42, 2)`  \n",
    "`(\"bar\", 7, 1)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce stage 1 completed in 0:00:04\n",
      "Emitted 904955 tuples\n"
     ]
    }
   ],
   "source": [
    "reduce_output = []\n",
    "\n",
    "t0 = time.time()\n",
    "output_count = 0\n",
    "\n",
    "keyfunc = lambda (w,d): (w,d)\n",
    "sorted_map_output = sorted(map_output, key=keyfunc)\n",
    "for (w,d), group in itertools.groupby(sorted_map_output, key=keyfunc):\n",
    "  reduce_output.append((w,d,len(list(group))))\n",
    "  output_count += 1\n",
    "  \n",
    "print \"Reduce stage 1 completed in %s\" % utils.pretty_timedelta(since=t0)\n",
    "print \"Emitted %d tuples\" % (output_count,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(u'!', 5327, 1),\n",
       " (u'!', 5354, 1),\n",
       " (u'!', 11541, 1),\n",
       " (u'\"', 0, 7),\n",
       " (u'\"', 6, 2),\n",
       " (u'\"', 9, 9),\n",
       " (u'\"', 10, 2),\n",
       " (u'\"', 12, 1),\n",
       " (u'\"', 13, 2),\n",
       " (u'\"', 14, 3)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduce_output[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reduce Stage 2\n",
    "\n",
    "In the second reduce stage, we'll build our actual word index. We'll store TF-IDF values, which we can use to compute relevance to a particular query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reduce stage 2 completed in 0:00:03\n",
      "Index size: 31077 words\n"
     ]
    }
   ],
   "source": [
    "doc_frequencies = {}\n",
    "tfidf_vectors = {}\n",
    "\n",
    "def tfidf(tf, df, N=doc_count):\n",
    "  \"\"\"Compute log-scaled TF-IDF.\"\"\"\n",
    "  return (1 + np.log10(tf)) * np.log10(float(N)/df)\n",
    "\n",
    "t0 = time.time()\n",
    "output_count = 0\n",
    "\n",
    "keyfunc = lambda (word, doc_id, tf): word\n",
    "sorted_reduce_output = sorted(reduce_output, key=keyfunc)\n",
    "for word, group in itertools.groupby(sorted_reduce_output, key=keyfunc):\n",
    "  posting_list = sorted([(d,tf) for (w,d,tf) in group])\n",
    "  df = len(posting_list)\n",
    "  \n",
    "  # Convert to TF-IDF score\n",
    "  tfidf_vec = [(d, tfidf(tf, df)) for (d,tf) in posting_list]\n",
    "  tfidf_vectors[word] = tfidf_vec\n",
    "  doc_frequencies[word] = df\n",
    "  \n",
    "  output_count += 1\n",
    "  \n",
    "print \"Reduce stage 2 completed in %s\" % utils.pretty_timedelta(since=t0)\n",
    "print \"Index size: %d words\" % (output_count,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 documents for term barley\n",
      "Sample entries (doc_id, tfidf_score):\n",
      "(460, 3.2582939505509438)\n",
      "(482, 2.2058405429751424)\n",
      "(483, 2.2058405429751424)\n",
      "(497, 2.8698647120623835)\n",
      "(529, 2.2058405429751424)\n",
      "(612, 2.2058405429751424)\n",
      "(615, 4.0699920624274197)\n",
      "(675, 2.8698647120623835)\n",
      "(1486, 2.2058405429751424)\n",
      "(1487, 2.2058405429751424)\n"
     ]
    }
   ],
   "source": [
    "term = \"barley\"\n",
    "print \"%d documents for term %s\" % (len(tfidf_vectors[term]), term)\n",
    "print \"Sample entries (doc_id, tfidf_score):\"\n",
    "print \"\\n\".join(map(str, tfidf_vectors[term][:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Index\n",
    "\n",
    "Now that we've got our TF-IDF vectors, we still need an efficient way to query the index. We'll exploit the fact that our vectors are still quite sparse, and only consider documents that have a match for at least one term in the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidates(query_words):\n",
    "  candidate_docs = collections.defaultdict(lambda: 0.0)\n",
    "  print \"Searching!\"\n",
    "  for word in query_words:\n",
    "    matches = tfidf_vectors[word]\n",
    "    idf = np.log10(float(doc_count)/doc_frequencies[word])\n",
    "    print \"- term \\\"%s\\\": %d documents, idf = %.03f\" % (word, len(matches), idf)\n",
    "    # Increment score for each matching doc\n",
    "    for (doc_id, score) in matches:\n",
    "      candidate_docs[doc_id] += score\n",
    "  \n",
    "  # Sort by most relevant\n",
    "  keyfunc = lambda (doc_id, score): score\n",
    "  return sorted(candidate_docs.iteritems(), key=keyfunc, reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten Blue Links\n",
    "\n",
    "Finally, we'll just add a bit of formatting code to make the output a bit more... [familiar](https://www.google.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching!\n",
      "- term \"barley\": 74 documents, idf = 2.206\n",
      "- term \"cyprus\": 11 documents, idf = 3.034\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "[0] <b><a href=\"results/result_0.html\" target=\"_blank\">CCC ACCEPTS BONUS BID ON BARLEY TO CYRPUS The Commodity Credit Corporation ( CCC ) has accepted a bid on ...</a></b>\n",
       "<br>\n",
       "document 8839 (relevance: 7.481)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[1] <b><a href=\"results/result_1.html\" target=\"_blank\">FRENCH CEREAL EXPORTS THROUGH ROUEN UP IN MARCH French cereals exports through Rouen port rose to 751 , 563 tonnes ...</a></b>\n",
       "<br>\n",
       "document 4757 (relevance: 6.568)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[2] <b><a href=\"results/result_2.html\" target=\"_blank\">USDA COMMENTS ON EXPORT SALES REPORT Corn sales of 2 , 806 , 300 tonnes in the week ended March ...</a></b>\n",
       "<br>\n",
       "document 3536 (relevance: 5.240)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[3] <b><a href=\"results/result_3.html\" target=\"_blank\">TALKING POINT / COPPER STOCKS Copper shares , which have lagged behind the market , should pick up steam this ...</a></b>\n",
       "<br>\n",
       "document 9004 (relevance: 4.860)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[4] <b><a href=\"results/result_4.html\" target=\"_blank\">CYPRUS MINERALS & lt ; CYPM > NAMED IN SUITS Cyprus Minerals Co said along with about 40 other companies ...</a></b>\n",
       "<br>\n",
       "document 6718 (relevance: 4.481)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[5] <b><a href=\"results/result_5.html\" target=\"_blank\">U . S . SUPPLY / DEMAND DETAILED BY USDA The U . S . Agriculture Department made the following ...</a></b>\n",
       "<br>\n",
       "document 615 (relevance: 4.070)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[6] <b><a href=\"results/result_6.html\" target=\"_blank\">CYPRUS LOWERS COPPER PRICE ONE CT TO 66 CTS Cyprus Minerals Company said it is decreasing its electrolytic copper cathode ...</a></b>\n",
       "<br>\n",
       "document 5406 (relevance: 3.947)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[7] <b><a href=\"results/result_7.html\" target=\"_blank\">PAPANDREOU SAYS GREEKS READY FOR AGGRESSORS Greek Prime Minister Andreas Papandreou said today that the Greek armed froces were ready ...</a></b>\n",
       "<br>\n",
       "document 3703 (relevance: 3.947)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[8] <b><a href=\"results/result_8.html\" target=\"_blank\">CYPRUS LOWERS COPPER PRICE 1 . 25 CTS TO 67 CTS Cyprus Minerals Company said it is decreasing its electrolytic ...</a></b>\n",
       "<br>\n",
       "document 4861 (relevance: 3.947)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "[9] <b><a href=\"results/result_9.html\" target=\"_blank\">EC WHEAT RELEASE UNLIKELY TO SATISFY U . K . DEMAND The European Commission ' s decision to release an ...</a></b>\n",
       "<br>\n",
       "document 5434 (relevance: 3.922)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "results_dir = \"results\"\n",
    "if not os.path.isdir(results_dir):\n",
    "  os.mkdir(results_dir)\n",
    "\n",
    "def ten_blue_links(query, k=10):\n",
    "  query_words = query.split()\n",
    "  candidates = get_candidates(query_words)\n",
    "  for i, (doc_id, score) in enumerate(candidates[:k]):\n",
    "    document = doc_index[doc_id]\n",
    "    # Write temp result file\n",
    "    fname = \"%s/result_%d.html\" % (results_dir, i)\n",
    "    with open(fname, 'w') as fd:\n",
    "      print >> fd, \" \".join(document)\n",
    "    # Display nice link format\n",
    "    link_text = \" \".join(document[:20]) + \" ...\"\n",
    "    utils.show_search_result(i, doc_id, score, fname, link_text)\n",
    "    \n",
    "ten_blue_links(\"barley cyprus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
