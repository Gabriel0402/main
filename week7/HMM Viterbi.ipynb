{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging with HMMs and Viterbi decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hidden Markov Models are often used to represent sequence data in NLP problems.  In this notebook, we use HMMs to model parts of speech of word sequences.\n",
    "\n",
    "A HMM models a system that consists of two parts:\n",
    "1.  A sequence of unobservable \"states\", $y_i$.  At each step, the system is in one of these.\n",
    "2.  A sequence of observable outputs (sometimes called \"emissions\"), $x_i$.  These outputs are produced as a result of the state the system is in at that step.\n",
    "\n",
    "<img src=\"hmm.png\">\n",
    "\n",
    "Recall that the \"Markov\" part of the HMMs acronym means that we are making some simplifying assumptions when modelling in this way:\n",
    "1.  The set of states are available for the next step (and with what distribution) depends only on the state at the current step.\n",
    "2.  The output generated at each step depends only on the state at that step.\n",
    "\n",
    "You can imagine that each node in the image of the HMM/graphical model above contains a conditional probability table that enumerates the probability distributions for the node given its \"parent\".\n",
    "\n",
    "In this notebook:\n",
    "1.  States correspond to parts of speech\n",
    "2.  Observable outputs are the words in the sentence.\n",
    "\n",
    "This aligns well with the HMM model definition above:  in any given sentence it's trivial to observe the words but the parts of speech are hidden.  This is called a \"generative model\" as the natural form of it allows you to run it forwards and \"simulate\" state transitions and corresponding emissions.\n",
    "\n",
    "This is similar to the language modelling we did earlier in the course (Weeks 2 and 4).  Just like in those cases, the reason we are doing this isn't to generate text.  Instead, we're using the model to \"score\" candidates.  In this case, these candidates are part of speech tag assignments.\n",
    "\n",
    "We'll describe the application in more detail when we get to the code below.  First, let's set things up by importing a few useful libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import treebank\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NLTK library only has ~10% of the Penn Treebank P.O.S. set.  It contains ~100K tokens consisting of ~12K types.  \n",
    "\n",
    "The treebank is available as part of the LDC Corpora access that you get by being a UC Berkeley student.  We use the NLTK subset here for convenience.\n",
    "\n",
    "These counts are big, but they aren't particularly huge.  As we learned in Week 2, the more data you have, the better estimates of probabilities you get.  Even with the full treebank, the smoothing techniques you learned about come in handy to deal with OOV words, rare transitions - or, more generally, sparsity.\n",
    "\n",
    "In this next cell, we dump the first 10 tokens and their part of speech tags to the output.  A [full list](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html) of these tags and their definitions was developed as part of the treebank project.  (There is no authoritative set of tags that everyone uses!  The Penn project had detailed specifications on how to tag words and parse sentences - something we'll learn about in a couple of weeks.)\n",
    "\n",
    "In the rest of this document, we refer to the tokens in the sentences as \"words\" and their part of speech tags as \"tags\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The treebank has 100676 tokens with 12408 types.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'Pierre', u'NNP'),\n",
       " (u'Vinken', u'NNP'),\n",
       " (u',', u','),\n",
       " (u'61', u'CD'),\n",
       " (u'years', u'NNS'),\n",
       " (u'old', u'JJ'),\n",
       " (u',', u','),\n",
       " (u'will', u'MD'),\n",
       " (u'join', u'VB'),\n",
       " (u'the', u'DT')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print 'The treebank has %d tokens with %d types.' % (len(treebank.tagged_words()), len(set(treebank.words())))\n",
    "treebank.tagged_words()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This usage of zip, if you aren't familiar with it, turns a list of tuples\n",
    "# \"inside out\":\n",
    "# l = [(1, 2), (3, 4), (5, 6)]\n",
    "# zip(*l) is [(1, 3, 5), (2, 4, 6)]\n",
    "#\n",
    "# In this case zip(*treebank.tagged_words()) results in\n",
    "# [(word1, word2, word3), (tag1, tag2, tag3)]\n",
    "\n",
    "# The sequence of part of speech tags corresponding with the sequence of\n",
    "# words in the corpus.\n",
    "tags = zip(*treebank.tagged_words())[1]\n",
    "\n",
    "# A (unique'd) set of all the tags available.\n",
    "tag_set = set(tags)\n",
    "\n",
    "# A histogram of tags.\n",
    "tag_count = Counter(tags)\n",
    "\n",
    "# A histogram of (word, tag) tuples.\n",
    "word_tag_count = Counter(treebank.tagged_words())\n",
    "\n",
    "# A histogram of (tag_next, tag_current) tuples.\n",
    "tag_tag_count = Counter(\n",
    "    (y2, y1) for y1, y2 in zip(tags[0:len(tags) - 1], tags[1:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the tags in the Penn dataset, along with their frequency.  A couple of these are pretty sparse.  The goal of this document is the clarity of Viterbi, not the performance of the algorithm, so we'll acknowledge that limitation and continue on anyways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'#': 16,\n",
       "         u'$': 724,\n",
       "         u\"''\": 694,\n",
       "         u',': 4886,\n",
       "         u'-LRB-': 120,\n",
       "         u'-NONE-': 6592,\n",
       "         u'-RRB-': 126,\n",
       "         u'.': 3874,\n",
       "         u':': 563,\n",
       "         u'CC': 2265,\n",
       "         u'CD': 3546,\n",
       "         u'DT': 8165,\n",
       "         u'EX': 88,\n",
       "         u'FW': 4,\n",
       "         u'IN': 9857,\n",
       "         u'JJ': 5834,\n",
       "         u'JJR': 381,\n",
       "         u'JJS': 182,\n",
       "         u'LS': 13,\n",
       "         u'MD': 927,\n",
       "         u'NN': 13166,\n",
       "         u'NNP': 9410,\n",
       "         u'NNPS': 244,\n",
       "         u'NNS': 6047,\n",
       "         u'PDT': 27,\n",
       "         u'POS': 824,\n",
       "         u'PRP': 1716,\n",
       "         u'PRP$': 766,\n",
       "         u'RB': 2822,\n",
       "         u'RBR': 136,\n",
       "         u'RBS': 35,\n",
       "         u'RP': 216,\n",
       "         u'SYM': 1,\n",
       "         u'TO': 2179,\n",
       "         u'UH': 3,\n",
       "         u'VB': 2554,\n",
       "         u'VBD': 3043,\n",
       "         u'VBG': 1460,\n",
       "         u'VBN': 2134,\n",
       "         u'VBP': 1321,\n",
       "         u'VBZ': 2125,\n",
       "         u'WDT': 445,\n",
       "         u'WP': 241,\n",
       "         u'WP$': 14,\n",
       "         u'WRB': 178,\n",
       "         u'``': 712})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the histograms to get the probability table (Pyy) for the hidden (y) nodes as well as the table (Pxy) for the observations.  Table Pab represents the value P(a | b).  Its key is a tuple (a, b) and its value is the probability value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Pxy = {(x, y): np.log(count) - np.log(tag_count[y])\n",
    "       for (x, y), count in word_tag_count.iteritems()}\n",
    "\n",
    "Pyy = {(y2, y1): np.log(count) - np.log(tag_count[y1])\n",
    "       for (y2, y1), count in tag_tag_count.iteritems()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we are at the viterbi algorithm implementation.\n",
    "\n",
    "The comments for this block are mostly inline, but a few observations before we get started:\n",
    "- To execute this code, you pass a list of words as sentence (e.g. 'hello world'.split(), not 'hello world')\n",
    "- verbose outputs the $\\delta$ table for inspection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def viterbi(sentence, verbose=False):\n",
    "    # How do we score out of vocabulary words?\n",
    "    # Obviously we should do something far more sophisticated (see week 2).\n",
    "    # The point of this notebook is clarity and so we just do some very\n",
    "    # unprincipled smoothing so we can focus\n",
    "    # on the rest of the implementation below.\n",
    "    if verbose:\n",
    "        EPS = -100\n",
    "    else:\n",
    "        EPS = -float('inf')\n",
    "        \n",
    "    # Delta is the Dynamic Programming table - or, more simply: the\n",
    "    # result cache.\n",
    "    # Its values are going to store the answer to the question:\n",
    "    # \"What is the best way to get to this word in the sentence where\n",
    "    # that word is tagged with that POS tag?\"\n",
    "    # \n",
    "    # As a result, delta's keys are tuples of the form\n",
    "    # (word_index_into_sentence, tag_of_word_at_index)\n",
    "    # \n",
    "    # Its values are (highest_logp_to_get_here, previous_tag_in_that_best_scoring_sequence)\n",
    "    #   We use the first half of the value tuple to track of the\n",
    "    #       \"score so far\".\n",
    "    #   We use the second half of the value tuple to track the\n",
    "    #       previous step in that optimal path.\n",
    "    delta = {}\n",
    "    \n",
    "    # Here, we prime the pump.  In lecture, we start with P(y1).\n",
    "    # This works, however, we might get a better\n",
    "    # estimate of the possible start states to a sentence if we use\n",
    "    # the transition probabilities from a period\n",
    "    # instead.  To that end, we pretend we have an extra hidden state\n",
    "    # whose value is deterministically the tag\n",
    "    # '.'.  Its score is 0.0 as that is log of a probability of 100%. \n",
    "    # We assign a epsilon probability to every other possible tag.\n",
    "    for tag in tag_set:\n",
    "        delta[(-1, tag)] = (EPS, '')\n",
    "    delta[(-1, '.')] = (0.0, '')\n",
    "    \n",
    "    # Here is the main work of the Viterbi algorithm...\n",
    "    # For each i=index, w=word in the sentence...\n",
    "    for i, x in enumerate(sentence):\n",
    "        # We consider and score every possible part of speech tag as\n",
    "        # an assignment to word x...\n",
    "        for y2 in tag_set:\n",
    "            # We determine which path through the previous word tag\n",
    "            # assignments is best when combined with\n",
    "            # assigning the current word (x) this tag (y2).  This\n",
    "            # answers the question: \"if you force me to\n",
    "            # assign tag y2 to word x, what is the best assignments to\n",
    "            # the previous words to make the whole\n",
    "            # collection as likely as possible?\"\n",
    "            # best_transition returns a tuple of\n",
    "            # (score_up_to_hidden_state_with_assignment_y2, previous_word's_tag)\n",
    "            best_transition = max(\n",
    "                [(delta[(i - 1, y1)][0] + Pyy.get((y2, y1), EPS), y1)\n",
    "                 for y1 in tag_set])\n",
    "            \n",
    "            # ... we then multiply in (add in log space) the\n",
    "            # probability of seeing word x given state y2\n",
    "            # and then set the entry in the delta table for the best\n",
    "            # score and path should we assign tag y2 to word x.\n",
    "            delta[(i, y2)] = (best_transition[0] + Pxy.get((x, y2), EPS), best_transition[1])\n",
    "\n",
    "    # Look through the assignment options to the last word.  Recall\n",
    "    # that the scores associated with those assignments are the score\n",
    "    # of the optimal sequence of assignments up until that\n",
    "    # point - in this case, the whole sentence.\n",
    "    #\n",
    "    # Pick the best scoring assignment.\n",
    "    decoding_tail = max(\n",
    "        [(delta[(len(sentence) - 1, tag)][0], tag)\n",
    "         for tag in tags])\n",
    "    \n",
    "    # (Interlude...) If requested, output the delta table for inspection.\n",
    "    if verbose:\n",
    "        for i in xrange(len(sentence) + 1):\n",
    "            for tag in sorted(tag_set):\n",
    "                if delta[i-1, tag][0] > 2 * EPS:\n",
    "                    print i - 1, tag, delta[i-1, tag]\n",
    "            print '\\n\\n'\n",
    "    # (...end interlude)\n",
    "\n",
    "    # Decode the sequence of tags from the table by following the\n",
    "    # \"previous\" links back through it, starting from the end.\n",
    "    decoding = [decoding_tail[1]]\n",
    "    for i in xrange(len(sentence)):\n",
    "        decoding.append(delta[(len(sentence) - i - 1, decoding[-1])][1])\n",
    "        \n",
    "    # Walking backwards gets the tags out in the wrong order... reverse.\n",
    "    return [tag for tag in reversed(decoding)][1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a sentence from the treebank and use Viterbi to decode.\n",
    "\n",
    "(A friendly reminder that given the amount of data we have, it's pretty easy to go out of vocabulary... but let's try a few sentences anyways.  First we'll grab a longish one from the training set.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pierre Vinken , 61 years old , will join the board as a nonexecutive director Nov. 29 .\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(u'NNP', u'NNP'),\n",
       " (u'NNP', u'NNP'),\n",
       " (u',', u','),\n",
       " (u'CD', u'CD'),\n",
       " (u'NNS', u'NNS'),\n",
       " (u'JJ', u'JJ'),\n",
       " (u',', u','),\n",
       " (u'MD', u'MD'),\n",
       " (u'VB', u'VB'),\n",
       " (u'DT', u'DT'),\n",
       " (u'NN', u'NN'),\n",
       " (u'IN', u'IN'),\n",
       " (u'DT', u'DT'),\n",
       " (u'JJ', u'JJ'),\n",
       " (u'NN', u'NN'),\n",
       " (u'NNP', u'NNP'),\n",
       " (u'CD', u'CD'),\n",
       " (u'.', u'.')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words, tags = zip(*[tagged_word for tagged_word in treebank.tagged_words()][:18])\n",
    "print ' '.join(words)\n",
    "zip(tags, viterbi(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'NN', u'VBZ', u'JJ']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('tea is tasty'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'PRP$', u'NN', u'VBZ', u'JJ']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('my computer is hot'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'VB', u'PRP', u'DT', u'NN']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('give me a sentence'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'WDT', u'VBD', u'IN']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('what was that'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'DT', u'NN']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('that car'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'DT', u'NN', u'VBD', u'IN', u'DT', u'NN']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('the food made by the man'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'DT', u'NNS', u'VB']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('the men walked'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'IN', u'VBD', u'JJ']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('that was weird'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'DT', u'NN', u'WDT', u'VBD', u'RB', u'JJ']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('the car that went too fast'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'DT', u'NN', u'WDT', u'VBD', u'NN', u'RB', u'JJ']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "viterbi('the car that went way too fast'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
